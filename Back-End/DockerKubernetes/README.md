# **Docker&Kubernetes** 🔄

### 참고자료

- 컨테이너 인프라 환경 구축을 위한 쿠버네티스/도커

### 학습 프로젝트 저장소

- [링크]()

## 목차
>- [새로운 인프라 환경이 온다](#새로운-인프라-환경이-온다)
>>- [컨테이너 인프라 환경이란](#컨테이너-인프라-환경이란)
>>- [컨테이너 인프라 환경을 지원하는 도구](#컨테이너-인프라-환경을-지원하는-도구)
>>- [새로운 인프라 환경의 시작](#새로운-인프라-환경의-시작)
>- [테스트 환경 구성하기](#테스트-환경-구성하기)
>>- [테스트 환경을 자동으로 구성하는 도구](#테스트-환경을-자동으로-구성하는-도구)
>>- [베이그런트로 랩 환경 구축하기](#베이그런트로-랩-환경-구축하기)
>>- [터미널 프로그램으로 가상 머신 접속하기](#터미널-프로그램으로-가상-머신-접속하기)
>- [컨테이너를 다루는 표준 아키텍처, 쿠버네티스](#컨테이너를-다루는-표준-아키텍처-쿠버네티스)
>>- [쿠버네티스 개요](#쿠버네티스-개요)
>>- [쿠버네티스 이해하기](#쿠버네티스-이해하기)
>>- [쿠버네티스 기본 사용법 배우기](#쿠버네티스-기본-사용법-배우기)
>>- [쿠버네티스 연결을 담당하는 서비스](#쿠버네티스-연결을-담당하는-서비스)
>>- [알아두면 쓸모 있는 쿠버네티스 오브젝트](#알아두면-쓸모-있는-쿠버네티스-오브젝트)
>- [쿠버네티스를 이루는 컨테이너 도우미, 도커](#쿠버네티스를-이루는-컨테이너-도우미-도커)
>>- [도커를 알아야 하는 이유](#도커를-알아야-하는-이유)
>>- [도커로 컨테이너 다루기](#도커로-컨테이너-다루기)
>>- [4가지 방법으로 컨테이너 이미지 만들기](#4가지-방법으로-컨테이너-이미지-만들기)
>>- [쿠버네티스에서 직접 만든 컨테이너 사용하기](#쿠버네티스에서-직접-만든-컨테이너-사용하기)


<br>

---

## 새로운 인프라 환경이 온다

>### 컨테이너 인프라 환경이란
>- 개요
>>- 컨테이너(container)는 하나의 운영체제 커널에서 다른 프로세스에 영향을 받지 않고 독립적으로 실행되는 프로세스 상태를 의미함
>>- 가상화 상태에서 동작하는 프로세스보다 가볍고 빠르게 동작함
>- 모놀리식 아키텍처
>>- 모놀리식 아키텍처(monolithic architecture)는 하나의 큰 목적이 있는 서비스 또는 애플리케이션에 여러 기능이 통합돼 있는 구조를 의미함
>>- 소프트웨어가 하나의 결합된 코드로 구성되기 때문에 초기 단계에서 설계하기 용이하며 개발지 좀 더 단순하고 코드 관리가 간편함
>>- 하지만 서비스를 운영하는 과정에서 수정이 많을 경우 연관된 다른 서비스에 영향을 미칠 가능성이 커지며, 서비스가 점점 성장해 기능이 추가될수록 처음에는 단순했던 서비스 간의 관계가 매우 복잡해질 수 있음
>- 마이크로서비스 아키텍처
>>- 마이크로서비스 아키텍처(MSA, Microservices Architecture)는 개별 기능을 하는 작은 서비스를 각각 개발해 연결하여 시스템 전체가 하나의 목적을 지향하는 구조임
>>- 개발된 서비스를 재사용하기 쉽고, 향후 서비스가 변경됐을 때 다른 서비스에 영향을 미칠 가능성이 줄어들며 사용량의 변화에 따라 특정 서비스만 확장할 수 있으므로 사용자의 요구사항에 따라 가용성을 즉각적으로 확보해야 하는 IaaS 환경에 적합함
>>- 하지만 모놀리식 아키텍처보다 복잡도가 높으며 각 서비스가 서로 유기적으로 통신하는 구조로 설계되기 때문에 네트워크를 통한 호출 횟수가 증가해 성능에 영향을 줄 수 있음

<br>

[목차로 이동](#목차)

>### 컨테이너 인프라 환경을 지원하는 도구
>- 개요
>>- 컨테이너 인프라 환경은 크게 컨테이너, 컨테이너 관리, 개발 환경 구성 및 배포 자동화, 모니터링으로 구성됨
>- 도커
>>- 도커(Docker)는 컨테이너 환경에서 도긻적으로 애플리케이션을 실행할 수 있도록 컨테이너를 만들고 관리하는 것을 도와주는 컨테이너 도구임
>>- 도커로 애플리케이션을 실행하면 운영체제 환경에 관계없이 독립적인 환경에서 일관된 결과를 보장함
>- 쿠버네티스
>>- 쿠버네티스(Kubernetes)는 다수의 컨테이너를 관리하는 데 사용함
>>- 컨테이너의 자동 배포와 배포된 컨테이너에 대한 동작 보증, 부하에 따른 동적 확장 등의 기능을 제공함
>>- 처음에는 다수의 컨테이너만 관리하는 도구였지만, 지금은 컨테이너 인프라에 필요한 기능을 통합하고 관리하는 솔루션으로 발전했음
>- 젠킨스
>>- 젠킨스(Jenkins)는 지속적 통합(CI, Continuous Integration)과 지속적 배포(CD, Continuous Deployment)를 지원함
>>- 지속적 통합과 지속적 배포는 개발한 프로그램의 빌드, 테스트, 패키지화, 배포 단계를 모두 자동화해 개발 단계를 표준화함
>- 프로메테우스와 그라파나
>>- 프로메테우스(Prometheus)와 그라파나(Grafana)는 모니터링을 위한 도구임
>>- 프로메테우스는 상태 데이터를 수집하고, 그라파나는 수집한 데이터를 관리자가 보기 좋게 시각화함
>>- 컨테이너 인프라 환경에서는 많은 종류의 소규모 기능이 각각 나누어 개발되기 때문에 중앙 모니터링이 필요함
>>- 프로메테우스와 그라파나는 컨테이너로 패키징돼 동작하며 최소한의 자원으로 쿠버네티스 클러스터의 상태를 시각적으로 표현함
>>- 데이터를 시각화하는 도구는 그라파나와 키바나(Kibana)가 시장을 양분한 상태이지만, 키바나는 프로메테우스와 연결 구성이 복잡하여 프로메테우스를 사용할 때는 간결하게 구성할 수 있는 그라파나가 더 선호됨

<br>

[목차로 이동](#목차)

---

## 테스트 환경 구성하기

>### 테스트 환경을 자동으로 구성하는 도구
>- 개요
>>- 코드로 인프라를 생성할 수 있게 지원하는 소프트웨어는 많지만 교육용 및 소규모 환경에선 베이그런트(Vagrant)가 가장 배우기 쉽고 사용 방법도 간단함
>>- 베이그런트는 가상화 소프트웨어인 버추얼 박스와도 호환성이 매우 좋음
>- 버추얼 박스 설치하기
>>- 버추얼 박스는 현존하는 대부분의 운영체제를 게스트 운영체제로 사용할 수 있으며 확장팩을 제외하면 아무런 제한 없이 소프트웨어의 모든 기능을 무료로 이용할 수 있음
>>- 다른 가상화 소프트웨어보다 기능이 강력하고 안정적임
>>- https://www.virtualbox.org/wiki/Download_Old_Builds_6_1 에서 6.1.12 버전으로 다운로드하고 기본 세팅으로 설치함
>- 베이그런트 설치하기
>>- 베이그런트는 프로비저닝(provisioning)해줌
>>>- 프로비저닝이란 사용자의 요구에 맞게 시스템 자원을 할당, 배치, 배포해 두었다가 필요할 때 시스템을 사용할 수 있는 상태로 미리 준비해두는 것
>>- https://www.vagrantup.com/downloads 에서 다운로드하고 기본 세팅으로 설치함
>- 베이그런트 구성하고 테스트하기
>>- 테스트 환경을 구성하기 전에 설치된 도구가 정상적으로 작동하는지 확인하기 위해 프로비저닝을 위한 코드를 작성하고, 이를 베이그런트에서 불러온 후 버추얼박스에 운영체제를 설치함
>>>1. cmd를 실행하고 베이그런트 설치 디렉토리로 이동후 vagrant init 명령을 실행하여 프로비저닝에 필요한 기본 코드를 생성함
>>>2. c:/HashiCorp 폴더의 Vagrantfile을 열어서 config.vm.box = "base" 라는 내용이 있는지 확인
>>>3. cmd에서 vagrant up 실행 후 설치하려는 이미지가 base로 명시되어 있으나 베이그런트가 해당 이미지를 찾지 못해 발생하는 에러 확인
>>>4. 가상 머신의 이미지를 선택하고 필요에 맞게 이미지를 수정하는 과정은 매우 복잡하고 어려우므로 저자가 목적에 맞게 제작해 둔 가상 이미지를 사용함
>>>>- 가상이미지는 베이그런트 클라우드(https://app.vagrantup.com/boxes/search)에 접속해 내려받는데, sysnet4admin/CentOS-k8s를 검색하여 확인해봄
>>>5. Vagrantfile의 config.vm.box ="sysnet4admin/CentOS-k8s" 로 수정함
>>>6. vagrant up 실행하여 해당 가상 머신 이미지를 내려받는지 확인함
>>>7. 버추얼박스를 실행하여 가상 머신이 제대로 생성됐는지 확인함
>>>8. cmd에서 vagrant ssh 실행하여 설치된 CentOS에 접속함
>>>9. uptime과 cat /etc/redhat-release 를 입력하여 설치가 정상적으로 이루어졌는지 확인함
>>>10. 설치 테스트가 완료되었다면 vagrant destroy -f 를 실행하여 가상 머신을 삭제함

<br>

[목차로 이동](#목차)

>### 베이그런트로 테스트 환경 구축하기
>- 가상 머신에 필요한 설정 자동으로 구성하기
>>1. 원하는 구성을 자동으로 생성할 수 있도록 Vagrantfile을 새로 작성, 베이그런트 코드는 루비로 작성됨
>>```ruby
>># -*- mode: ruby -*-
>># vi: set ft=ruby :
>>Vagrant.configure("2") do |config|
>>  config.vm.define "m-k8s" do |cfg|
>>  config.vm.boot_timeout = 1800
>>    cfg.vm.box = "sysnet4admin/CentOS-k8s"
>>    cfg.vm.provider "virtualbox" do |vb|
>>      vb.name = "m-k8s(github_SysNet4Admin)"
>>      vb.cpus = 1
>>      vb.memory = 1024
>>      vb.customize ["modifyvm", :id, "--groups", "/k8s-SM(github_SysNet4Admin)"]
>>    end
>>    cfg.vm.host_name = "m-k8s"
>>    cfg.vm.network "private_network", ip: "192.168.1.10"
>>    cfg.vm.network "forwarded_port", guest: 22, host: 60010, auto_correct: true, id: "ssh"
>>    cfg.vm.synced_folder "../data", "/vagrant", disabled: true
>>  end
>>end
>>```
>>2. vagrant up 실행 후 vagrant ssh 로 접속
>>3. ip addr show eth1 실행하여 ip가 192.168.1.10 으로 설정됐는지 확인
>>4. exit 로 CentOS 접속 종료
>- 가상 머신에 추가 패키지 설치하기
>>1. Vagrantfile 에 셸 프로비전을 추가
>>```ruby
>>...
>>cfg.vm.synced_folder "../data", "/vagrant", disabled: true
>>cfg.vm.provision "shell", path: "install_pkg.sh" #add provisioning script
>>```
>>2. Vagrantfile 이 위치한 디렉토리에 추가 패키지를 설치하기 위한 스크립트인 install_pkg.sh 를 작성함
>>```sh
>>#!/usr/bin/env bash
>># install packages
>>yum install epel-release -y
>>yum install vim-enhanced -y
>>```
>>3. vagrant provision 명령으로 추가한 프로비전 구문을 실행함
>>4. vagrant ssh 명령으로 CentOS에 접속함
>>5. yum repolist 명령으로 추가한 EPEL 저장소가 구성됐는지 확인함
>>6. vi .bashrc 명령으로 문법 하이라이트가 적용됐는지 확인함
>>7. 모든 내용을 확인했다면 exit 후 vagrant destroy -f 명령으로 가상 머신을 삭제함
>- 가상 머신 추가로 구성하기
>>- 베이그런트로 운영 체제를 자동으로 설치하고 구성하면 편리하지만 단순히 운영 체제 1개를 구성하려고 사용하는 것은 아님
>>- 기존에 설치한 가상 머신 외에 가상 머신 3대를 추가로 설치하고 기존의 가상 머신과 네트워크 통신이 원활하게 작동하는지 확인함
>>- Vagrantfile
>>```ruby
>># -*- mode: ruby -*-
>># vi: set ft=ruby :
>>Vagrant.configure("2") do |config|
>>  config.vm.define "m-k8s" do |cfg|
>>  config.vm.boot_timeout = 1800
>>    cfg.vm.box = "sysnet4admin/CentOS-k8s"
>>    cfg.vm.provider "virtualbox" do |vb|
>>      vb.name = "m-k8s(github_SysNet4Admin)"
>>      vb.cpus = 2
>>      vb.memory = 2048
>>      vb.customize ["modifyvm", :id, "--groups", "/k8s-SM(github_SysNet4Admin)"]
>>    end
>>    cfg.vm.host_name = "m-k8s"
>>    cfg.vm.network "private_network", ip: "192.168.1.10"
>>    cfg.vm.network "forwarded_port", guest: 22, host: 60010, auto_correct: true, id: "ssh"
>>    cfg.vm.synced_folder "../data", "/vagrant", disabled: true
>>    cfg.vm.provision "shell", path: "install_pkg.sh" #add provisioning script
>>    cfg.vm.provision "file", source: "ping_2_nds.sh", destination: "ping_2_nds.sh"
>>    cfg.vm.provision "shell", path: "config.sh"
>>  end
>>
>>  #=============#
>>  # Added Nodes #
>>  #=============#
>>  (1..3).each do |i|
>>    config.vm.define "w#{i}-k8s" do |cfg|
>>    config.vm.boot_timeout = 1800
>>      cfg.vm.box = "sysnet4admin/CentOS-k8s"
>>      cfg.vm.provider "virtualbox" do |vb|
>>        vb.name ="w#{i}-k8s(github_SysNet4Admin)"
>>        vb.cpus = 1
>>        vb.memory = 1024
>>        vb.customize ["modifyvm", :id, "--groups", "/k8s-SM(github_SysNet4Admin)"]
>>      end
>>      cfg.vm.host_name = "w#{i}-k8s"
>>      cfg.vm.network "private_network", ip: "192.168.1.10#{i}"
>>      cfg.vm.network "forwarded_port", guest: 22, host: "6010#{i}", auto_correct: true, id: "ssh"
>>      cfg.vm.synced_folder "../data", "/vagrant", disabled: true
>>      cfg.vm.provision "shell", path: "install_pkg.sh"
>>    end
>>  end
>>end
>>```
>>- ping_2_nds.sh
>>```sh
>># ping 3 times per nodes
>>ping 192.168.1.101 -c 3
>>ping 192.168.1.102 -c 3
>>ping 192.168.1.103 -c 3
>>```
>>- config.sh
>>```sh
>>#!/usr/bin/env bash
>># modify permission
>>chmod 744 ./ping_2_nds.sh
>>```
>>1. vagrant up 명령으로 총 4대의 CentOS를 설치하고 구성함
>>2. vagrant ssh 명령으로 설치된 CentOS에 접속 시도시 설치된 가상 머신이 여러 대이기 때문에 접속할 가상 머신의 이름을 입력해야 한다는 메시지가 출력되므로 vagrant ssh m-k8s 명령을 실행함
>>3. Vagrantfile 에 작성된 스크립트에 따라 업로드된 ./ping_2_nds.sh 파일을 실행하여 3대의 CentOS와 통신하는 데 문제가 없는지 확인함
>>4. 실행에 이상이 없다면 exit 명령으로 가상 머신 접속을 종료

<br>

[목차로 이동](#목차)

>### 터미널 프로그램으로 가상 머신 접속하기
>- 푸티 설치하기
>>- 푸티(PuTTY란 터미널 접속 프로그램 중에서 가장 많이 사용되는 프로그램임
>>- 가볍고 무료이며 다양한 플러그인을 통해 여러 대의 가상 머신에 접속할 수 있으며 접속 정보를 저장하고 바로 불러와 실행할 수 있는 기능이 있음
>>- https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html 에서 Alternative binary files 항목에서 운영 체제 및 버전에 맞는 실행 파일을 다운로드함
>- 슈퍼푸티 설치하기
>>- 푸티를 단독으로 사용하면 창을 여러 개 띄워야 하므로 명령을 내리기가 매우 번거롭다는 문제가 있는데 슈퍼푸티를 사용하여 해결이 가능함
>>- https://github.com/jimradford/superputty/releases/tag/1.4.0.9 에서 SuperPuttySetup-1.4.0.9.msi 를 다운로드 받고 실행하여 기본옵션으로 설치함
>>- 설치완료 후 실행하여 putty.exe Location (Required) 옆의 Browse 버튼을 클릭하여 푸티의 위치를 지정함
>- 슈퍼푸티로 다수의 가상 머신 접속하기
>>1. 슈퍼푸티 화면 오른쪽에 위치한 Sessions 창의 PuTTY Sessions 에서 마우스 오른쪽 버튼을 클릭하고 New Folder를 클릭함
>>2. 접속 정보 입력 창에서 k8s를 입력하고 OK 클릭함
>>3. 새로 추가된 k8s 디렉토리에서 마우스 오른쪽 버튼을 클릭하고 New 클릭
>>4. 가상 머신의 정보를 입력하는 창에 Session Name에 m-k8s, Host Name에 127.0.0.1, TCP Port에 60010, Login Username에 root, Extra PuTTY Arguments에 -pw vagrant 입력 후 Save 하면 Arguments 가 평문으로 저장되어 보안상 위험하다는 창이 뜨는데 확인 클릭함
>>5. m-k8s 에서 마우스 오른쪽 버튼을 클릭하여 Copy As로 복사하고 Session Name에 w1-k8s, TCP Port에 60101 입력함
>>6. m-k8s 에서 마우스 오른쪽 버튼을 클릭하여 Copy As로 복사하고 Session Name에 w2-k8s, TCP Port에 60102 입력함
>>7. m-k8s 에서 마우스 오른쪽 버튼을 클릭하여 Copy As로 복사하고 Session Name에 w3-k8s, TCP Port에 60103 입력함
>>8. 평문으로 접속하기 위해 슈퍼푸티의 보안 설정을 변경, Tools > Options 의 GUI 탭에서 Allow plain text passwords on putty command line 항목을 체크하고 OK
>>9. k8s 디렉토리에서 마우스 오른쪽 버튼을 누르고 Connect All을 선택하여 모든 가상 머신에 한번에 접속함
>>10. 슈퍼푸티가 푸티를 호출하면서 경고창이 뜨는데 실행을 누르고 추가로 발생하는 보안경고에서도 예를 클릭함
>>11. 가상 머신에 접속되는지 확인하고 창을 분리배치하여 작동을 한눈에 확인할 수 있음
>>12. 상단의 commands 창에서 hostname을 입력하여 4개의 창에서 모두 실행되는지 확인함
>>13. 이상이 없다면 vagrant destroy -f 로 가상 머신을 모두 삭제함

<br>

[목차로 이동](#목차)

---

## 컨테이너를 다루는 표준 아키텍처, 쿠버네티스

>### 쿠버네티스 개요
>- 컨테이너 인프라 환경
>>- 리눅스 운영 체제의 커널 하나에서 여러 개의 컨테이너가 격리된 상태로 실행되는 인프라 환경임
>>- 컨테이너는 하나 이상의 목적을 위해 독립적으로 작동하는 프로세스임
>>- 개인 환경에서는 1명의 관리자가 다양한 응용 프로그램을 사용하므로 각각의 프로그램을 컨테이너로 구현할 필요가 없으나 기업 환경에서는 다수의 관리자가 수백 또는 수천 대의 서버를 함께 관리하기 때문에 일관성을 유지하는 것이 매우 중요하기 때문에 컨테이너 인프라 환경을 구성 시 눈송이 서버(여러 사람이 만져서 설정의 일관성이 떨어진 서버)를 방지하는 데 효과적임
>>- 가상화 환경에서는 각각 의 가상 머신이 모두 독립적인 운영 체제 커널을 가지고 있어야 하기 때문에 자원을 더 소모해야 하고 성능이 떨어질 수 밖에 없으나 컨테이너 인프라 환경은 운영 체제 커널 하나에 컨테이너 여러 개가 격리된 형태로 실행되기 때문에 자원을 효율적으로 사용할 수 있고 거치는 단계가 적어서 속도도 훨씬 빠름
>- 쿠버네티스의 등장
>>- 가상화 환경에서 상용 솔루션(VMware)을 이용해 안정적으로 시스템을 운용하고 있었고, 기숭 성숙도가 높아 문제없이 관리되고 있었으나 커널을 공유해 더 많은 애플리케이션을 올릴 수 있는 컨테이너가 도입되기 시작하면서 늘어난 컨테이너를 관리해야 했으나 기존의 컨테이너 관리 솔루션들은 현업의 요구 사항을 충족시키기에는 부족하여 컨테이너 인프라 환경의 장점이 많음에도 불구하고 보편화되기 어려웠음
>>- 구글이 2014년 자사에서 컨테이너 운영 플랫폼으로 운영하던 보그(Borg)를 기반으로하는 쿠버네티스(Kubernetes)를 오픈 소스화 하고 2015년 7월 21일 쿠버네티스 버전 1.0을 출시함과 동시에 리눅스 재단과 파트너 쉽을 맺고 클라우드 컴퓨팅 재단을 설립하여 쿠버네티스를 기초 기술로 제공함
>>- 안정적인 쿠버네티스를 누구나 자유롭게 이용하게 되면서 컨테이너 인프라 환경을 좀 더 효율적으로 관리할 수 있게 되었고, 이후 여러 기능이 추가되며 쿠버네티스의 생태계가 풍부해짐에 따라 사실상 쿠버네티스는 컨테이너 인프라 관리 솔루션의 표준이 됨

<br>

[목차로 이동](#목차)

>### 쿠버네티스 이해하기
>- 쿠버네티스
>>- 컨테이너 관리 도구로 설명되지만, 실제로는 컨테이너 오케스트레이션을 위한 솔루션임
>>>- 오케스트레이션(Orchestration)이란 복잡한 단계를 관리하고 요소들의 유기적인 관계를 미리 정의해 손쉽게 사용하도록 서비스를 제공하는 것을 의미함
>>- 컨테이너 오케스트레이션이란 다수의 컨테이너를 유기적으로 연결, 실행, 종료할 뿐만 아니라 상태를 추적하고 보존하는 등 컨테이너를 안정적으로 사용할 수 있게 만들어주는 것임
>- 왜 쿠버네티스인가
>>- 도커 스웜은 간단하게 설치할 수 있고 사용하기도 용이하나 기능이 다양하지 않아 대규모 환경에 적용하려면 사용자 환경을 변경해야할 수 있으므로 소규모 환경에서는 유용하지만 대규모 환경에서는 잘 사용하지 않음
>>- 쿠버네티스는 시작하는 데 어려움이 있지만 쉽게사용할 수 있도록 도와주는 도구들이 있어 설치가 쉬워지는 추세이며 다양한 형태의 쿠버네티스가 지속적으로 계속 발전되고 있어서 커네티언 오케스트레이션을 넘어 IT 인프라 자체를 컨테이너화하고, 컨테이너화된 인프라 제품군을 쿠버네티스 위에서 동작할 수 있게 만듬
>>- 거의 모든 벤더와 오픈 소스 진영 모두에서 쿠버네티스를 지원하고 그에 맞게 통합 개발하고 있으므로 컨테이너 오케스트레이션을 학습하거나 도입하려고 한다면 쿠버네티스를 최우선적으로 고려해야 함
>>- k8s 란 k8(ubernete, 8글자)s 의 형식으로 그리스어로 도선사(pilot, 배를 수로로 안전하게 안내하는 사람)나 조타수(helmsman, 배의 키를 조정해 올바른 방향으로 나아가게 하는 사람)을 의미함
>- 구성방법
>>- 퍼블릭 클라우드 업체에서 제공하는 관리형 쿠버네티스인 EKS, AKS, GKE 등이 있으며 구성이 이미 갖춰져 있고 마스터 노드를 클라우드 입장에서 관리하기 때문에 학습용으로는 적합하지 않음
>>- 수세의 Rancher, 레드햇의 OpenShift와 같은 플랫폼에서 제공하는 설치형 쿠버네티스가 있으나 유료라 쉽게 접근하기 어려움
>>- 구성형 쿠버네티스 즉, 사용하는 시스템에 쿠버네티스 클러스터를 자동으로 구성해주는 솔루션을 사용할 수 있으며, 주요 솔루션으로 kubeadm, kops, KRIB, kubespray가 있고 kubeadm이 가장 널리 알려져 있음
>>>- kubeadm은 사용자가 변경하기도 수월하고, 온프레미스와 클라우드를 모두 지원하며 배우기도 쉬움
>- 구성하기
>>- https://github.com/sysnet4admin/_Book_k8sInfra 에서 다운로드 받아 C:/HashiCorp 폴더로 옮겨 압축풀기
>>- C:\HashiCorp\_Book_k8sInfra-main\ch3\3.1.3
>>>- config.sh : kubeadm 으로 쿠버네티스를 설치하기 위한 사전 조건을 설정하는 스크립트 파일
>>>- install_pkg.sh : 클러스터를 구성하기 위해서 가상 머신에 설치돼야 하는 의존성 패키지를 명시하고 실습에 필요한 소스 코드를 특정 가상 머신(m-k8s) 내부에 내려받도록 설정함
>>>- master_node.sh : m-k8s 가상 머신을 쿠버네티스 마스터 노드로 구성하는 스크립트이며 쿠버네티스 클러스터를 구성할 때 꼭 선택해야 하는 컨테이너 네트워크 인터페이스(CNI)도 함께 구성함
>>>- work_nodes.sh : 3대의 가상 머신에 쿠버네티스 워커 노드를 구성하는 스크립트이며 마스터 노드에 구성된 클러스터에 조인이 필요한 정보가 모두 코드화돼 있어 스크립트를 실행하기만 하면 편하게 워커 1로서 쿠버네티스 클러스터에 조인됨
>>- 해당 경로에서 vagrant up 명령으로 쿠버네티스 클러스터를 자동으로 구성하고, 완료되면 슈퍼푸티로 m-k8s에 접속함
>>- kubectl get nodes 명령으로 쿠버네티크 클러스터에 마스터 노드와 워커 노드들이 정상적으로 생성되고 연결됐는지 확인함
>- 파드 배포를 중심으로 쿠버네티스 구성 요소 살펴보기
>>- 개요
>>>- kubectl, kubelet, API 서버, 캘리코 등은 모두 쿠버네티스 클러스터를 이루는 구성요소이며 이외에도 etcd, 컨트롤러 매니저, 스케줄러, kube-proxy, 컨테이너 런타임, 파드 등이 있음
>>>- 쿠버네티스 클러스터를 이루는 구성 요소들은 파드 형태로 이루어져 있으며 kubectl get pods --all-namespaces 명령으로 확인할 수 있음
>>>- 쿠버네티스의 구성 요소는 동시에 여러 개가 존재하는 경우 중복된 이름을 피하기 위해 뒤에 해시 코드가 삽입되며 무작위 문자열로 생성됨
>>- 관리자나 개발자가 파드를 배포할 때 파드를 배포하는 순서에 따라 요소들의 역할
>>>- 마스터 노드
>>>>1. kubectl : 쿠버네티스 클러스터에 명령을 내리는 역할을 하며 다른 구성 요소들과 다르게 바로 실행되는 명령 형태인 바이너리로 배포되기 때문에 마스터 노드에 있을 필요는 없으나 통상적으로 API 서버와 주로 통신하므로 해당 도서에서는 API 서버가 위치한 마스터 노드에 구성함
>>>>2. API 서버 : 쿠버네티스 클러스터의 중심 역할을 하는 통로로서 주로 상태 값을 저장하는 etcd 와 통신하지만 그 밖의 요소들 또한 API 서버를 중심에 두고 통신하므로 매우 중요함, 회사로 비유하면 모든 직원과 상황을 관리하고 목표를 설정하는 관리자에 해당함
>>>>3. etcd(etc + distributed) : 구성 요소들의 상태 값이 모두 저장되는 곳으로서 회사의 관리자가 모든 보고 내용을 기록하는 노트라고 생각할 수 있음
>>>>4. 컨트롤러 매니저 : 쿠버네티스 클러스터의 오브젝트 상태를 관리함
>>>>>- 워커 노드에서 통신이 되지 않는 경우, 상태 체크와 복구는 컨트롤러 매니저에 속한 노드 컨트롤러에서 이루어짐
>>>>5. 스케줄러 : 노드의 상태와 자원, 레이블, 요구 조건 등을 고려해 파드를 어떤 워커 노드에 생성할 것인지를 결정하고 할당함
>>>- 워커 노드
>>>>6. kubelet : 파드의 구성 내용(PodSpec)을 받아 컨테이너 런타임으로 전달하고, 파드 안의 컨테이너들이 정상적으로 작동하는지 모니터링함
>>>>7. 컨테이너 런타임(CRI, Container Runtime Interface) : 파드를 이루는 컨테이너의 실행을 담당하며 파드 안에서 다양한 종류의 컨테이너가 문제 없이 작동하게 만드는 표준 인터페이스임
>>>>8. 파드(Pod) : 한 개 이상의 컨테이너로 단일 목적의 일을 하기 위해서 모인 단위임
>>>>>- 웹 서버 역할을 할 수도 있고 로그나 데이터를 분석할 수도 있는데, 중요한 것은 언제라도 죽을 수 있는 존재임
>>>>>- 가장 이해하기 어려운 부분이며, 가상 머신은 언제라도 죽을 수 있다고 가정하고 디자인하지 않지만, 파드는 언제라도 죽을 수 있다고 가정하고 설계됐기 때문에 쿠버네티스는 여러 대안을 디자인 했음
>>>- 선택 가능한 구성 요소
>>>>9. 네트워크 플러그인 : 쿠버네티스 클러스터의 통신을 위해서 네트워크 플러그인을 선택하고 구성해야 하는데, 일반적으로 CNI로 구성하며 캘리코, 플래널, 실리움, 큐브라우터, 로마나, 위브넷, Canal이 있음
>>>>10. CoreDNS : 클라우드 네이티브 컴퓨팅 재단에서 보증하는 프로젝트로, 빠르고 유연한 DNS 서버임
>>>>>- 쿠버네티스 클러스터에서 도메인 이름을 이용해 통신하는 데 사용하며, 실무에서 쿠버네티스 클러스터를 구성하여 사용할 때는 IP보다 도메인 네임을 편리하게 관리해주는 CoreDNS를 사용하는 것이 일반적임
>>- 사용자가 배포된 파드에 접속할 때
>>>- 파드가 배포된 이후 사용자 입장에서 배포된 파드에 접속하는 과정
>>>1. kube-proxy : 쿠버네티스 클러스터는 파드가 위치한 노드에 kube-proxy 를 통해 파드가 통신할 수 있는 네트워크를 설정함
>>>>- 실제 통신은 br_netfilter 와 iptables 로 관리하며 Vagrantfile에서 호출하는 config.sh 에서 확인할 수 있음
>>>2. 파드 : 이미 배포된 파드에 접속하고 필요한 내용을 전달받는데, 대부분 사용자는 파드가 어느 워커 노드에 위치하는지 신경 쓸 필요 없음
>- 파드의 생명주기로 쿠버네티스 구성 요소 살펴보기
>>- 개요
>>>- 쿠버네티스의 가장 큰 장점은 쿠버네티스의 구성 요소마다 하는 일이 명확하게 구분돼 각자의 역할만 충실하게 수행하면 클러스터 시스템에 안정적으로 운영된다는 점임
>>>- 각자의 역할이 명확하게 나눠진 것은 마이크로서비스 아키텍처 구조와도 밀접하게 연관됨
>>>- 역할이 나뉘어 있어 문제가 발생했을 때 어느 부분에서 문제가 발생했는지 디버깅하기 쉬움
>>- 파드의 생명주기
>>>1. kubectl 을 통해 API 서버에 파드 생성을 요청함
>>>2. (업데이트가 있을 때마다 매번) API 서버에 전달된 내용이 있으면 API 서버는 etcd에 전달된 내용을 모두 기록해 클러스터의 상태 값을 최신으로 유지하므로 각 요소가 상태를 업데이트할 때마다 모두 API 서버를 통해 etcd에 기록됨
>>>3. API 서버에 파드 생성이 요청된 것을 컨트롤러 매니저가 인지하면 컨트롤러 매니저는 파드를 생성하고, 이 상태를 API 서버에 전달함, 추가로 아직 어떤 워커 노드에 파드를 적용할지는 결정되지 않은 상태로 파드만 생성됨
>>>4. API 서버에 파드가 생성됐다는 정보를 스케줄러가 인지하고 스케줄러는 생성된 파드를 어떤 워커 노드에 적용할지 조건을 고려해 결정하고 해당 워커 노드에 파드를 띄우도록 요청함
>>>5. API 서버에 전달된 정보대로 지정한 워커 노드에 파드가 속해 있는지 스케줄러가 kubelet 으로 확인함
>>>6. kubelet 에서 컨테이너 런타임으로 파드 생성을 요청함
>>>7. 파드가 생성됨
>>>8. 파드가 사용 가능한 상태가 됨
>- 쿠버네티스 구성 요소의 기능 검증하기
>>- kubectl 를 어디서나 실행하는 방법
>>>1. 슈퍼푸티로 w3-k8s 접속 후 kubectl get nodes 명령 시 포트 관련 에러 발생
>>>>- 쿠버네티스 클러스터의 정보를 kubectl이 알지 못하기 때문에 쿠버네티스의 노드들에 대한 정보가 표시되지 않음
>>>>- kubectl은 API 서버를 통해 쿠버네티스에 명령을 내리기때문에 어디에 있더라도 API의 서버의 접속 정보만 있다면 쿠버네티스 클러스터에 명령을 내릴 수 있음
>>>2. 쿠버네티스 클러스터의 정보(/etc/kubernetes/admin.conf)를 마스터 노드에서 scp(secure sopy) 명령으로 w3-k8s 의 현재 디렉터리(.)에 받아오며, 이때 접속 기록이 없기 때문에 known_hosts로 저장하도록 yes를 입력하고, 마스터 노드의 접속 암호인 vagrant도 입력함
>>>>- scp root@192.168.1.10:/etc/kubernetes/admin.conf .
>>>3. kubectl get nodes --kubeconfig admin.conf 명령으로 노드 정보 확인
>>- kubelet 는 쿠버네티스에서 파드의 생성과 상태 관리 및 복구 등을 담당하는 매우 중요한 구성요 요소로, 문제가 생기면 파드가 정상적으로 관리되지 않음
>>>1. 기능 검증을 위해 파드를 배포함
>>>>- m-k8s에서 kubectl create -f ~/_Book_k8sInfra/ch3/3.1.6/nginx-pod.yaml 명령으로 nginx 웹 서버 파드를 배포함
>>>>- -f 옵션은 일반적으로 쓰는 force가 아니라 filename을 의미함. 즉, 파드의 구성 내용을 파일로 읽어 들여 1개의 파드를 임의의 워커 노드에 배포하는 것임
>>>2. kubectl get pod 명령으로 배포된 파드가 정상적으로 배포된 상태(Running)인지 확인함
>>>3. kubectl get pods -o wide 명령으로 파드가 배포된 워커 노드를 확인함
>>>>- -o 는 output의 약어로 출력을 특정 형식으로 해 주는 옵션이며, wide는 제공되는 출력 형식 중에서 출력 정보를 더 많이 표시해 주는 옵션임
>>>4. 배포된 노드에 접속하여 systemctl stop kubelet 으로 서비스를 멈춤
>>>5. m-k8s로 돌아와서 kubectl get pod로 상태를 확인하고 kubectl delete pod nginx-pod 명령으로 파드를 삭제함, 하지만 시간이 지나도 변화가 없으므로 Ctrl_c로 중지함
>>>6. kubectl get pod 명령으로 파드의 상태를 확인하면 nginx-pod를 삭제(Terminating)하는 중으로 출력되지만, kubelet이 작동하지 않는 상태이기 때문에 삭제되지 않음
>>>7. 워커 노드에서 systemctl start kubelet 으로 복구하고 마스터 노드에서 kubectl get pod 으로 상태를 확인하면 삭제되어 있음
>>- kube-proxy 는 파드의 통신을 담당하는데, config.sh 의 설정이 정상적으로 작동하지 않는다면 생기는 문제 확인
>>>1. 마스터 노드에서 kubectl create -f ~/_Book_k8sInfra/ch3/3.1.6/nginx-pod.yaml 으로 다시 파드를 배포함
>>>2. kubectl get pod -o wide 명령으로 파드의 IP와 워커 노드를 확인하고 "curl 파드의 IP" 명령으로 nginx 웹 서버 메인 페이지 내용을 확인함
>>>3. 확인한 워쿼 노드에서 modprobe -r br_netfilter 명령으로 파드가 위치한 워커 노드에서 br_netfilter 모듈을 제거함
>>>>- -r 은 remove를 의미하며, 명령 실행 후 systemctl restart network 명령으로 네트워크를 다시 시작하여 변경된 내용을 적용하여 kube-proxy 에 문제가 생기는 상황을 만듬
>>>4. 마스터 노드에서 curl로 nginx 웹 서버 페이지 정보를 받아오는 명령을 수행해도 명령이 완료되지 않으므로 Ctrl+c 로 종료
>>>5. 워커 노드에서 modprobe br_netfilter 명령으로 br_netfilter 를 커널에 적재하고 시스템을 다시 시작하여 적용함
>>>6. 일정 시간 이후 마스터 노드에서 "kubectl get pod -o wide" 명령을 실행하면 restarts가 1 증가하고 IP가 변경된것을 확인함
>>>7. 해당 ip에서 curl 을 실행하여 파드로부터 정보를 정상적으로 받아오는지 확인함
>>>8. kubectl delete -f ~/_Book_k8sInfra/ch3/3.1.6/nginx-pod.yaml 명령으로 다음 내용 진행을 위해 배포한 파드를 삭제함

<br>

[목차로 이동](#목차)

>### 쿠버네티스 기본 사용법 배우기
>- 파드를 생성하는 방법
>>- 파드를 간단하게 생성하는 방법
>>>- kubectl run nginx-pod --image=nginx
>>- 기존 방법
>>>- kubectl create nginx --image=nginx -> 에러
>>>- kubectl create deployment dpy-nginx --image=nginx -> 성공
>>>>- 생성된 파드는 이름 뒤에 무작위로 해쉬코드가 붙음
>>- run 으로 생성시 단일 파드 1개만 생성되고 관리되지만, create deployment로 생성 시 디플로이먼트라는 관리 그룹 내에서 파드가 생성됨
>- 오브젝트란
>>- 개요
>>>- 쿠버네티스를 사용하는 관점에서 피드와 디플로이먼트는 스펙과 상태 등의 값을 가지고 있음
>>>- 스펙과 상태 등의 값을 가지고 있는 파드와 디플로이먼트를 개별 속성을 포함해 부르는 단위를 오브젝트라고 함
>>>- 쿠버네티스는 여러 유형의 오브젝트를 제공함
>>- 기본 오브젝트
>>>1. 파드
>>>>- 쿠버네티스에서 실행되는 최소 단위, 즉 웹 서비스를 구동하는 데 필요한 최소 단위임
>>>>- 독립적인 공간과 사용 가능한 IP를 가지고 있음
>>>>- 하나의 파드는 1개 이상의 컨테이너를 갖고 있기 때문에 여러 기능을 묶어 하나의 목적으로 사용할 수도 있음
>>>>- 범용으로 사용할 때는 대부분 1개의 파드에 1개의 컨테이너를 적용함
>>>2. 네임스페이스
>>>>- 쿠버네티스 클러스터에서 사용되는 리소스들을 구분해 관리하는 그룹임
>>>>- 해당 장에서는 기본으로 할당되는 default, 쿠버네티스 시스템에서 사용되는 kube-system, 온프레미스에서 쿠버네티스를 사용할 경우 외부에서 쿠버네티스 클러스터 내부로 접속하게 도와주는 컨테이너들이 속해 있는 metallb-system 3가지 네임스페이스를 사용함
>>>3. 볼륨
>>>>- 파드가 생성될 때 파드에서 사용할 수 있는 디렉토리를 제공함
>>>>- 기본적으로 파드는 영속되는 개념이 아니라 제공되는 디렉토리도 임시로 사용함
>>>>- 파드가 사라지더라도 저장과 보존이 가능한 디렉토리를 볼륨 오브젝트를 통해 생성하고 사용할 수 있음
>>>4. 서비스
>>>>- 파드는 클러스터 내에서 유동적이기 때문에 접속 정보가 고정일 수 없으므로 파드 접속을 안정적으로 유지하도록 서비스를 통해 내/외부로 연결됨
>>>>- 서비스는 새로 파드가 생성될 때 부여되는 새로운 IP를 기존에 제공하던 기능과 연결해줌
>>- 디플로이먼트
>>>- 기본 오브젝트만으로도 쿠버네티스를 사용할 수 있으나 한계가 있으므로 좀 더 효율적으로 작동하도록 기능들을 조합하고 추가해 구현한 것이 디플로이먼트임
>>>- 이외에도 데몬셋, 컨피그맵, 레플리카셋, PV, PVC, 스테이트풀셋 등 앞으로도 요구 사항에 따라 목적에 맞는 오브젝트들이 추가됨
>>>- 쿠버네티스에서 가장 많이 쓰이는 디플로이먼트 오브젝트는 파드에 기반을 두고 있으며, 레플리카셋 오브젝트를 합쳐 놓은 형태임
>>- 쿠버네티스에서 사용하는 NGINX 이미지를 가져오는 장소
>>>- 컨테이너로 도커를 사용하므로 도커의 기본 저장소인 도커 허브(https://hub.docker.com/_/nginx) 에서 이미지를 가지고 옴
>>>- 도커 허브에 사용자의 고유 사설 저장소를 만들어서 사용할 수도 있음
>>- 디플로이먼트를 생성하고 삭제
>>>- kubectl create deployment dpy-hname --image=sysnet4admin/echo-hname 으로 디플로이먼트 생성
>>>- kubectl get pods 으로 생성되었는지 확인
>>>- kubectl delete deployment dpy-hname 으로 삭제
>>>- kubectl get pods 으로 삭제되었는지 확인
>- 레플리카셋으로 파드 수 관리하기
>>- 개요
>>>- 많은 사용자를 대상으로 웹 서비스를 하려면 다수의 파드가 필요한데, 하나씩 생성한다면 매우 비효율적이므로 쿠버네티스에서는 다수의 파드를 만드는 레플리카셋 오브젝트를 제공함
>>- 레플리카셋이 파드의 개수를 지정한 대로 맞춰주는 과정
>>>1. kubectl get pods 명령으로 배포된 파드의 상태를 확인
>>>2. kubectl scale pod nginx-pod --replicas=3 명령으로 파드로 생성된 nginx-pod 는 디플로이먼트 오브젝트가 아님을 에러를 통해 확인함
>>>3. kubectl scale deployment dpy-nginx --replicas=3 명령으로 디플로이먼트로 생성된 dpy-nginx 파드의 수를 3개로 만듬
>>>4. kubectl get pods -o wide 명령으로 모든 파드가 정상적으로 워커 노드에 적용되고 IP가 부여됐는지 확인
>>>5. kubectl delete deployment dpy-nginx 명령으로 디플로이먼트를 삭제함
>>>6. kubectl get pods 명령으로 삭제되었는지 확인
>- 스펙을 지정해 오브젝트 생성하기
>>- 개요
>>>- kubectl create deployment 명령으로 디플로이먼트를 생성할 수 있지만 1개의 파드만 만들어지므로 생성시 여러 개의 파드를 만들 수 있는 방법이 필요함
>>>- create 에서는 replicas 옵션을 사용할 수 없고 scale은 이미 만들어진 디플로이먼트에서만 사용할 수 있음
>>>- 설정을 적용하기 위해선 필요한 내용을 파일로 작성해야 하며 작성하는 파일을 오브젝트 스펙이라고 하며 일반적으로 야믈(YAML, Yet Another Markup Language, 또 다른 마크업 언어 였으나 공식 사이트에선 YAML Ain't Markup Language, 야믈은 단순한 마크업 언어가 아니다로 재정의함)문법으로 작성함
>>- 3개의 nginx 파드를 디플로이먼트 오브젝트로 만들기 위해 오브젝트 스펙을 작성
>>```yaml
>>apiVersion: apps/v1 # API 버전, kubectl api-versions 명령으로 쿠버네티스에서 사용 가능한 API 버전을 확인할 수 있음
>>kind: Deployment # 오브젝트 종류
>>metadata:
>>  name: echo-hname  # 디플로이먼트의 이름
>>  labels: # 디플로이먼트의 레이블
>>    app: nginx
>>spec:
>>  replicas: 3 # 몇 개의 피드를 생성할지 결정
>>  selector: # 셀렉터의 레이블 지정
>>    matchLabels:
>>      app: nginx
>>  template: # 템플릿의 레이블 지정
>>    metadata:
>>      labels:
>>        app: nginx
>>    spec: # 템플릿에서 사용할 컨테이너 이미지 지정
>>      containers:
>>      - name: echo-hname
>>        image: sysnet4admin/echo-hname # 사용되는 이미지
>>```
>>>- 쿠버네티스는 API 버전마다 포함되는 오브젝트도 다르고 요구하는 내용도 다르므로 처음부터 모든 내용을 숙지하기보다 기존에 만들어진 파일을 수정하면서 이해해 보고 필요한 내용을 그때마다 정리하는 것이 좋음
>>- 스펙을 이용한 디플로이먼트 생성
>>>1. kubectl create -f ~/_Book_k8sInfra/ch3/3.2.4/echo-hname.yaml 명령을 실행하여 디플로이먼트를 생성함
>>>2. kubectl get pods 로 파드가 3개인지 확인함
>>>3. sed -i 's/replicas: 3/replicas: 6/' ~/_Book_k8sInfra/ch3/3.2.4/echo-hname.yaml 으로 스펙파일의 replicas의 값을 3으로 변경함
>>>>- sed 는 streamlined editor 를 의미하며 -i 는 --in-place 의 약어로 변경한 내용을 현재 파일에 바로 적용하겠다는 의미이며, s/는 주어진 패턴을 원하는 패턴으로 변경하겠다는 의미임
>>>>- vim으로 직접 파일을 수정해도 상관없음
>>>4. cat ~/_Book_k8sInfra/ch3/3.2.4/echo-hname.yaml | grep replicas 명령으로 값이 변경되었는지 확인함
>>>5. kubectl create -f ~/_Book_k8sInfra/ch3/3.2.4/echo-hname.yaml 명령으로 변경된 내용을 적용시도하면 에러 메시지 출력되는것을 확인
>>>>- 파일을 변경후 이미 배포된 오브젝트의 스펙을 변경하는 방법이 필요함!
>- apply로 오브젝트 생성하고 관리하기
>>- 개요
>>>- run 은 파드를 간단하게 생성하는 매우 편리한 방법이지만 단일 파드만을 생성할 수 있어 모든 상황에 적용해 사용하기는 어려움
>>>- create 로 디플로이먼트를 생성하면 파일의 변경 사항을 바로 적용할 수 없다는 단점이 있음
>>>- 쿠버네티스는 apply 라는 명령어를 제공하여 오브젝트를 관리할 수 있음
>>- apply 적용 예시
>>>1. kubectl applyu -f ~/_Book_k8sInfra/ch3/3.2.4/echo-hname.yaml 명령을 실행하면 오브젝트를 처음부터 apply로 생성한 것이 아니므로 경고가 출력됨
>>>>- 작동에는 문제가 없지만 일관성에서 문제가 생길 수 있으므로 변경 사항이 발생할 가능성이 있는 오브젝트는 처음부터 apply로 생성하는 것이 좋음
>>>2. kubectl get pods 명령으로 파드가 6개로 늘어났는지, 그리고 AGE를 확인하여 최근에 추가된 파드가 3개인지 확인함
>- 파드의 컨테이너 자동 복구 방법
>>- 셀프힐링
>>>- 쿠버네티스는 거의 모든 부분이 자동 복구되도록 설계돼어 있는데 특히 파드의 자동 복구 기술을 셀프 힐링이라고 함
>>>- 제대로 작동하지 않는 컨테이너를 다시 시작하거나 교체해 파드가 정상적으로 작동하게 함
>>- 셀프힐링 테스트
>>>1. 파드에 접속하기위해 kubectl get pods -o wide 명령으로 파드의 IP를 확인
>>>2. kubectl exec -it nginx-pod -- /bin/bash 명령으로 배시 셸에 접속
>>>>- -i 는 stdin(표준 입력)이고, t는 tty(teletypewriter를 뜻하며 합쳐서 표준 입력을 명령줄 인터페이스로 작성한다는 의미임
>>>>- 파드인 nginx-pod에 /bin/bash를 실행하여 nginx-pod의 컨테이너에서 배시 셸에 접속한다는 의미임
>>>3. ls -l /run/nginx.pid 명령으로 프로세스가 생성된 시간을 확인함
>>>4. i=1; while true; do sleep 1; echo $((i++)) `curl --silent 172.16.221.129 | grep title` ; done
>>>>- 슈퍼푸티에서 m-k8s 터미널을 하나 더 띄우고 해당 터미널 화면에서 nginx-pod의 IP에서 돌아가는 웹 페이지를 1초마다 한 번씩 요청하는 스크립트를 실행함
>>>>- curl 에서 요청한 값만 받도록 --slient 옵션을 추가함
>>>5. 배시 셸에서 nginx 프로세서인 PID 1번을 kill로 종료함
>>>6. 스크립트를 실행한 터미널에서 스크립트가 자동으로 복구되는지 확인함
>>>7. ls -l /run/nginx.pid 명령으로 프로세스의 생성 시간이 변경되었는지 확인함
>- 파드의 동작 보증 기능
>>- 쿠버네티스는 파드 자체에 문제가 발생하면 파드를 자동 복구해서 파드가 항상 동작하도록 보장하는 기능이 있음
>>- 기능 테스트 과정
>>>1. kubectl get pods 명령으로 파드들의 목록을 확인함
>>>2. kubectl delete pods nginx-pod 로 삭제함
>>>3. kubectl delete pods echo-hname-아무거나하나 명령을 실행하면 기존 nginx-pod를 삭제할 때보다 오래걸림
>>>4. kubectl get pods 명령으로 삭제되었는지 확인하면 삭제한 파드는 사라지고 새로운 파드가 생성된 것을 확인함
>>>>- nginx-pod는 디플로이먼트에 속한 파드가 아니며 어떤 컨트롤러도 이 파드를 관리하지 않으므로 바로 삭제되고 다시 생성되지도 않음
>>>>- echo-hname은 디플로이먼트에 속한 파드이므로 삭제되더라도 replicas 의 개수대로 다시 생성됨
>>>5. kubectl delete deployment echo-hname 명령으로 디플로이먼트 삭제
>>>6. kubectl get pods 명령으로 배포된 파드가 남아있는지 확인
>- 노드 자원 보호하기 
>>- 개요
>>>- 노드는 쿠버네티스 스케줄러에서 파드를 할당받고 처리하는 역할을 함 (마스터노드, 워커노드)
>>>- 최근에 몇 차례 문제가 생긴 노드에 파드를 할당하면 문제가 생길 가능성이 높지만 어쩔 수 없이 해당 노드를 사용해야하는 경우 영향도가 적은 파드를 할당하여 일정 기간 사용하면서 모니터링 하여 파드의 문제를 최소화해야함
>>>- 쿠버네티스는 모든 노드에 균등하게 파드를 할당하려고 하기 때문에 문제가 생길 가능성이 있는 노드라는 것은 쿠버네티스에 알려야함
>>>- 쿠버네티스에서는 이런 경우에 cordon 기능을 사용하여 노드를 관리함
>>- cordon 기능 테스트 과정
>>>1. kubectl apply -f ~/_Book_k8sInfra/ch3/3.2.8/echo-hname.yaml 명령으로 파드 생성
>>>2. kubectl scale deployment echo-hname --replicas=9 명령으로 배포한 파드를 9개로 늘림
>>>3. kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName 명령으로 배포된 9개의 파드가 제대로 작동하는지, IP 할당이 잘 됐는지, 각 노드로 공평하게 배분됐는지를 확인함
>>>>- -o 는 output을 의미함
>>>>- custom-columns 는 사용자가 임의로 구성할 수 있는 열의 의미함
>>>>- NAME, IP, STATUS, NODE는 열의 제목이고, 콜론 뒤는 .metadata.name, .status.podIP, .status.phase, .spec.nodeName 내용 값임
>>>4. kubectl scale deployment echo-hname --replicas=3 명령으로 다시 3개로 줄임
>>>5. kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName 명령으로 각 노드에 파드가 1개씩만 남았는지 확인함
>>>6. kubectl cordon w3-k8s 명령으로 문제가 자주 발생하므로 현재 상태를 보존하도록함
>>>>- 해당 노드에 더 이상 파드가 할당되지 않게 스케줄 되지 않는 상태(SchedulingDisabled)라는 표시를 함
>>>7. kubectl get nodes 명령으로 cordon 명령이 제대로 적용됐는지 확인함
>>>8. kubectl scale deployment echo-hname --replicas=9 명령으로 파드 수를 9개로 늘림
>>>9. kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName 명령으로 w3-k8s 에 파드가 추가 할당되지 않았음을 확인함
>>>10. kubectl scale deployment echo-hname --replicas=3 명령으로 다시 3개로 줄임
>>>11. kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName 명령으로 각 노드에 할당된 파드 수가 1개씩인지 확인함
>>>12. kubectl uncordon w3-k8s 명령으로 파드가 할당되지 않게 설정했던 것을 해제함
>>>13. kubectl get nodes 명령으로 uncordon이 적용됐는지 확인함
>- 노드 유지보수하기
>>- 정기 또는 비정기적인 유지보수를 위해 노드를 꺼야 하는 상황을 대비하여 쿠버네티스는 지정된 노드의 파드를 전부 다른 곳으로 이동시켜 해당 노드를 유지보수할 수 있게 하는 drain 기능을 제공함
>>- drain 기능 테스트 과정
>>>1. kubectl drain w3-k8s 명령으로 유지보수할 노드(w3-k8s)를 파드가 없는 상태로 만들려고 하면 데몬셋을 지울 수 없어 명령을 수행할 수 없다고 출력됨
>>>>- drain은 실제로 파드를 옮기는 것이 아니라 노드에서 파드를 삭제하고 다른 곳에 다시 생성하는 것이기 때문에 DaemonSet은 각 노드에 1개만 존재하는 파드라서 drain으로는 삭제할 수 없음
>>>2. kubectl drain w3-k8s --ignore-daemonsets 명령으로 DaemonSet을 무시하고 진행하면 경고가 발생하지만 모든 파드가 이동됨
>>>3. kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName 명령으로 w3-k8s에 파드가 없는지 확인하고 다른 파드로 옮겨졌는지 확인함
>>>4. kubectl get nodes 명령으로 w3-k8s 노드가 Ready,SchedulingDisabled 상태인지 확인함
>>>5. kubectl uncordon w3-k8s 명령으로 스케줄을 받을 수 있는 상태로 복귀시킴
>>>6. kubectl get nodes 명령으로 노드 상태를 확인함
>>>7. kubectl  delete -f ~/_Book_k8sInfra/ch3/3.2.8/echo-hname.yaml 명령으로 다음 진행을 위해 삭제함
>>>8. kubectl get pods 명령으로 배포된 파드가 없는지 확인함
>- 파드 업데이트하고 복구하기
>>- 파드를 운영하다 보면 컨테이너에 새로운 기능을 추가하거나 치명적인 버그가 발생해 버전을 업데이트 하거나, 업데이트 도중 문제가 발생하여 다시 기존 버전으로 복구해야하는 일도 쿠버네티스에선 처리할 수 있음
>>- 파드 업데이트 하기
>>>1. kubectl apply -f ~/_Book_k8sInfra/ch3/3.2.10/rollout-nginx.yaml --record 명령으로 컨테이너 버전 업데이트를 테스트하기 위한 파드를 배포함
>>>>- --record 배포한 정보의 히스토리를 기록하는 매우 중요한 옵션임
>>>2. kubectl rollout history deployment rollout-nginx 명령으로 record 옵션으로 기록된 히스토리를 확인할 수 있음
>>>3. kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName 명령으로 배포한 파드의 정보를 확인함
>>>4. curl -I --silent 파드중하나의IP | grep Server 명령으로 배포된 파드에 속해 있는 nginx 컨테이너 버전을 확인함
>>>5. kubectl set image deployment rollout-nginx nginx=nginx:1.16.0 --record 명령으로 nginx 컨테이너 버전을 업데이트함
>>>6. kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName 명령으로 업데이트 후 파드의 상태를 확인함
>>>>- 파드들의 이름과 IP가 변경됨
>>>>- 파드에 속한 컨테이너를 업데이트하는 가장 쉬운 방법은 파드를 관리하는 replicas의 수를 줄이고 늘려 파드를 새로 생성하는 것이고, 시스템의 영향을 최소화하기 위해 replicas에 속한 파드를 모두 한 번에 지우는 것이 아니라 파드를 하나씩 순차적으로 지우고 생성함
>>>>- 파드 수가 많으면 하나씩이 아니라 다수의 파드가 업데이트되며 업데이트 기본값은 전체의 1/4(25%)개 이며, 최솟값은 1개임
>>>7. kubectl rollout history deployment rollout-nginx 명령으로 rollout-nginx 에 적용된 명령들을 확인함
>>>8. curl -I --silent 파드중하나의IP | grep Server 명령으로 업데이트가 제대로 이루어졌는지 확인함
>>- 업데이트 실패 시 파드 복구하기
>>>1. kubectl set image deployment rollout-nginx nginx=nginx:1.17.23 --record 명령으로 의도(1.17.2)와 다르게 입력함
>>>2. kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName 명령으로 상태 확인 시 파드가 삭제되지 않고 pending(대기중) 상태에서 넘어가지 않는 것을 확인함
>>>3. kubectl rollout status deployment rollout-nginx 명령으로 새로운 replicas는 생성했으나 디플로이먼트를 배포하는 단계에서 대기 중이므로 더 이상 진행하지 않은 것을 확인함
>>>4. kubectl rollout status deployment rollout-nginx 명령을 시간을 좀 두고 다시 실행하면 여러 번 시도했지만 끝내 생성되지 않았다는 메시지를 확인함
>>>5. kubectl describe deployment rollout-nginx 명령으로 쿠버네티스의 상태를 살펴볼 수 있음
>>>>- 1.17.23 버전의 nginx 컨테이너가 없기 때문에 컨테이너 이미지를 찾지 못해 디플로이먼트가 배포되지 않아 replicas가 새로 생성되는 과정에서 멈춰있음
>>>6. kubectl rollout history deployment rollout-nginx 명령으로 업데이트할 때 사용했던 명령들을 확인
>>>7. kubectl rollout undo deployment rollout-nginx 명령으로 실행을 취소하여 마지막 단계(revision 3)에서 전 단계(revision 2)로 상태를 되돌림
>>>8. kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName 명령으로 복구된 파드 상태를 확인함
>>>9. kubectl rollout history deployment rollout-nginx 명령으로 revision 4 가 추가되고 revision 2 가 삭제된것을 확인함
>>>10. curl -I --silent 파드중하나의IP | grep Server 명령으로 nginx 버전 확인함
>>>11. kubectl rollout status deployment rollout-nginx 명령으로 변경이 정상적으로 적용됐는지 확인함
>>>12. kubectl describe deployment rollout-nginx 명령으로 현재 디플로이먼트 상태도 세부적으로 점검함
>- 특정 시점으로 파드 복구하기
>>- 바로 전 상태가 아닌 특정 시점으로 복구하고 싶다면 --to-revision 옵션을 사용함
>>>1. kubectl rollout undo deployment rollout-nginx --to-revision=1 명령으로 처음 상태인 revision 1 으로 복구함
>>>2. kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName 명령으로 새로 생성된 파드들의 IP를 확인함
>>>3. curl -I --silent 파드중하나의IP | grep Server 명령으로 nginx 컨테이너 버전이 1.15.12 버전임을 확인함
>>>4. kubectl delete -f ~/_Book_k8sInfra/ch3/3.2.10/rollout-nginx.yaml 명령으로 다음 단계 진행을 위해 디플로이먼트를 삭제함
>>>5. kubectl get pods 명령으로 배포된 파드가 없는 것을 확인함

<br>

[목차로 이동](#목차)

>### 쿠버네티스 연결을 담당하는 서비스
>- 개요
>>- 쿠버네티스 클러스터 내부에서만 파드를 사용했으나 외부 사용자가 파드를 이용하는 방법도 존재함
>>- 일반적으로 서비스는 웹 서비스나 네트워크 서비스 같은 운영 체제에 속한 서비스 데몬 또는 개발 중인 서비스를 생각하기 쉬우나 쿠버네티스에서는 외부에서 쿠버네티스 클러스터에 접속하는 방법을 서비스라고 함
>- 가장 간단하게 연결하는 노드포트
>>- 개요
>>>- 외부에서 쿠버네티스 클러스의 내부에 접속하는 가장 쉬운 방법은 노드포트 서비스를 이용하는 것임
>>>- 노트포트 서비스를 설정하면 모든 워커 노드의 특정 포트(노드 포트)를 열고 여기로 오는 모든 요청을 노드포트 서비스로 전달함
>>>- 노트포트 서비스는 해당 업무를 처리할 수 있는 파드로 요청을 전달함
>>- 노트포트 서비스로 외부에서 접속하기
>>>1. kubectl create deployment np-pods --image=sysnet4admin/echo-hname 명령으로 디플로이먼트로 파드를 생성함
>>>2. kubectl get pods 명령으로 배포된 파드를 확인함
>>>3. kubectl create -f ~/_Book_k8sInfra/ch3/3.3.1/nodeport.yaml 명령으로 노트포트 서비스를 생성하는데 오브젝트의 세부 스팩은 파일로 확인함
>>>4. kubectl get services 명령으로 노트포트 서비스로 생성한 np-svc 서비스를 확인함
>>>5. kubectl get nodes -o wide 명령으로 쿠버네티스 클러스터의 워커 노드 IP를 확인함
>>>6. 확인한 워커 노드의 IP:30000 을 웹 브라우저를 통해 접속하여 외부에서 접속되는지 확인함
>>- 부하 분산 테스트하기
>>>- 디플로이먼트로 생성된 파드 1개에 접속하고 있는 현 상황에서 파드가 3개로 증가하면 로드밸런서 기능으로 부하가 분산되는지 확인함
>>>1. 파워셸 명령으로 반복적으로 접속해 접속한 파드 이름을 화면에 표시함
>>>```shell
>>>$i=0; while($true)
>>>{
>>> % { $i++; write-host -NoNewline "$i $_" }
>>> (Invoke-RestMethod "http://192.168.1.101:30000")-replace '\n', " "
>>>}
>>>```
>>>2. 파워셸로 코드를 실행 후 쿠버네티스 마스터 노드에서 kubectl scale deployment np-pods --replicas=3 명령으로 파드를 증가시킴
>>>3. kubectl get pods 명령으로 배포된 파드를 확인함
>>>4. 파워셸 명령 창에서 표시되는 파드 이름으로 배포된 파드 3개가 돌아가면서 표시되는지 확인하여 부하 분산이 제대로 되는지 확인함
>>- expose 로 노드포트 서비스 생성하기
>>>- 노트포트 서비스는 오브젝트 스펙 파일로도 생성가능하고, expose 명령어로도 생성할 수 있음
>>>1. kubectl expose deployment np-pods --type=NodePort --name=np-svc-v2 --port=80 명령으로 서비스로 내보낼 디플로이먼트를 np-pods로 지정함
>>>>- 서비스의 이름은 np-svc-v2, 타입은 NodePort, 서비스가 파드로 보내줄 연결 포트를 80번으로 지정함
>>>2. kubectl get services 명령으로 생성된 서비스를 확인하여 웹 브라우저에서 접속하여 배포된 파드 중 하나의 이름이 웹 브라우저에 접속되는지 확인함
>>>3. 다음 실습 진행을 위해 kubectl delete deployment np-pods, kubectl delete services np-svc, kubectl delete services np-svc-v2 각각의 명령을 실행하여 배포한 디플로이먼트와 서비스를 삭제함
>- 사용 목적별로 연결하는 인그레스
>>- 개요
>>>- *******************154p ~ 207p 내용은 프로젝트 기간 제한으로 인해 빠른 도커 및 젠킨스 학습이 필요하여 잠시 유보

<br>

[목차로 이동](#목차)

>### 알아두면 쓸모 있는 쿠버네티스 오브젝트
>- 

<br>

[목차로 이동](#목차)

---

## 쿠버네티스를 이루는 컨테이너 도우미, 도커

>### 도커를 알아야 하는 이유
>- 파드, 컨테이너, 도커, 쿠버네티스의 관계
>>- 파드들은 워커 노드라는 노드 단위로 관리하며, 워커 노드와 마스터 노드가 모여 쿠버네티스 클러스터가 됨
>>- 파드는 1개 이상의 컨테이너로 이루어져 있으며 쿠버네티스로부터 IP를 받아 컨테이너가 외부와 통신할 수 있는 경로를 제공하며 컨테이너들이 정상적으로 작동하는지 확인하고 네트워크나 저장 공간을 서로 공유하게 함
>>- 파드가 이러한 환경을 만들기 때문에 컨테이너들은 마치 하나의 호스트에 존재하는 것처럼 작동할 수 있음
>>- 구조를 이루는 가장 기본인 컨테이너는 하나의 운영 체제 안에서 커널을 공유하며 개별적인 실행 환경을 제공하는 격리된 공간임
>>>- 개별적인 실행 환경이란 CPU, 네트워크, 메모리와 같은 시스템 자원을 독자적으로 사용하도록 할당된 환경을 말함
>>>- 개별적인 실행 환경에서는 실행되는 프로세스를 구분하는 ID도 컨테이너 안에 격리돼 관리되므로 각 컨테이너 내부에서 실행되는 애플리케이션들은 서로 영향을 미치지 않고 독립적으로 작동할 수 있음
>>- 각 컨테이너가 독립적으로 작동하기 때문에 여러 컨테이너를 효과적으로 다룰 방법이 필요하며, 과거부터 유닉스와 리눅스는 프로세스를 격리해서 관리하는 방법을 제공했으나 일부 전문가만 사용할정도로 복잡했던 과정을 쉽게 만들어주는 도구가 도커임
>>>- 도커는 컨테이너의 사용하는 방법을 명령어로 정리한 것이라고 생각하면 쉬우며, 도커를 사용하면 사용자가 따로 신경쓰지 않아도 컨테이너를 생성할 때 개별적인 실행 환경을 분리하고 자원을 할당함

<br>

[목차로 이동](#목차)

>### 도커로 컨테이너 다루기
>- 개요
>>- 컨테이너 이미지와 컨테이너의 관계
>>>- 컨테이너 이미지는 이미지 자체로는 사용할 수 없고 도커와 같은 CRI로 불러들여야 컨테이너가 실제로 작동함
>>>- 컨테이너를 삭제할 때는 내려받은 이미지와 이미 실행된 컨테이너를 모두 삭제해야만 디스크의 용량을 온전히 확보할 수 있음
>- 컨테이너 이미지 알아보기
>>- 이미지 검색하고 내려받기
>>>- 이미지는 레지스트리라고 하는 저장소에 모여있으며 레지스트리는 도커 허브(https://hub.docker.com)처럼 공개된 유명 레지스트리일 수도 있고, 내부에 구축한 레지스트리일 수도 있음
>>>- 이미지는 레지스트리 웹 사이트에서 직접 검색해도 되고, 슈퍼푸티 명령 창에서 쿠버네티스 마스터 노드에 접속해 검색할 수도 있음, 이때 별도의 레지스트리를 지정하지 않으면 기본적으로 도커 허브에서 이미지를 찾음
>>>- docker search nginx 처럼 입력하면 특정한 이름을 포함하는 이미지가 있는지 찾음
>>>>- INDEX : 이미지가 저장된 레지스트리의 이름
>>>>- NAME : 검색된 이미지 이름
>>>>- DESCRIPTION : 이미지에 대한 설명
>>>>- STARS : 해당 이미지를 내려받은 사용자에게 받은 평가 횟수
>>>>- OFFICIAL : [OK] 표시는 해당 이미지에 포함된 애플리케이션, 미들웨어 등을 개발한 업체에서 공식적으로 제공한 이미지라는 의미
>>>>- AUTOMATED : [OK] 표시는 도커 허브에서 자체적으로 제공하는 이미지 빌드 자동화 기능을 활용해 생성한 이미지를 의미함
>>>- docker search 로 찾은 이미지는 docker pull 로 내려받을 수 있음
>>>>- 태그 : Using default tag 와 함께 뒤에 따라오는 태그 이름을 통해 이미지를 내려받을때 사용한 태그를 알 수 있으며 아무런 조건을 주지 않고 이미지 이름만으로 pull을 수행하면 기본으로 latest 태그가 적용됨
>>>>- 레이어 : Pull complete 메시지 앞의 해쉬 코드가 pull 을 수행하여 내려받은 레이어이며 하나의 이미지는 여러 개의 레이어로 이루어져 있음
>>>>- 다이제스트 : 이미지의 고유 식별자로, 이미지에 포함된 내용과 이미지의 생성 환경을 식별할 수 있음, 식별자는 해시 함수로 생성되며 이미지가 동일한지 검증하는 데 사용됨
>>>>- 상태 : 이미지를 내려받은 레지스트리, 이미지, 태그 등의 상태 정보를 확인할 수 있음
>>- 이미지 태그
>>>- 태그는 이름이 동일한 이미지에 추가하는 식별자로 도커 이미지의 버전이나 플랫폼을 구분하는 데 사용함
>>>- 이미지를 내려받거나 이미지를 기반으로 컨테이너를 구동할 때는 이미지 이름만 사용하고 태그를 명시하지 않으면 latest 태그를 기본으로 사용함
>>- 이미지의 레이어 구조
>>>- 컨테이너 이미지를 실행 파일이라고 했으나 사실 이미지는 애플리케이션과 각종 파일을 담고 있다는 점에서 ZIP 같은 압축 파일에 더 가까움
>>>- 압축파 파일은 압축한 파일의 개수에 따라 전체 용량이 증가하지만 이미지는 같은 내용일 경우 여러 이미지에 동일한 레이어를 공유하므로 전체 용량이 감소함
>>>- 두 개의 nginx 이미지 (latest와 stable)을 비교하여 차이점을 살펴보며 이미지의 레이어 구조를 확인하는 실습
>>>>1. docker pull nginx:stable 명령으로 stable 이미지를 내려받음
>>>>>- 실행 결과의 Already exists 를 통해 같은 부분이 있으면 레이어를 공유한다는 것을 확인할 수 있음
>>>>2. docker images nginx 명령으로 내려받은 이미지를 조회함
>>>>>- stable 태그와 latest 태그의 두 개 이미지가 검색되고 각각 SIZE를 확인할 수 있는데, 두 이미지가 공유하는 레이어가 있다면 실제 차지하는 용량은 적음
>>>>3. docker history nginx:stable 과 docker history nginx:latest 명령으로 생성 과정에서 단계별로 용량을 얼마나 차지하는지 자세한 이력을 확인할 수 있으며 ADD file 부분으로 두 이미지가 공유하는 레이어의 크기를 확인할 수 있음
>- 컨테이너 실행하기
>>- 컨테이너를 단순히 실행하는 실습과정
>>>1. docker run -d --restart always nginx 명령으로 새로운 컨테이너를 실행함
>>>>- 실행 시 컨테이너를 식별할 수 있는 고유한 ID가 출력됨
>>>>- -d(--detach) 옵션은 컨테이너를 백그라운드에서 구동한다는 의미이며 생략 시 컨테이너 내부에서 실행되는 애플리케이션의 상태가 화면에 계속 표시됨
>>>>- --restart always 옵션은 컨테이너의 재시작과 관련된 정책을 의미하는 옵션으로 프로그램에 예상하지 못한 오류가 발생하거나 리눅스 시스템에서 도커 서비스가 중지되는 경우에 컨테이너도 작동이 중지되는데 이때 중지된 컨테이너를 즉시 재시작하거나 리눅스 시스템에서 도커 서비스가 작동할 때 컨테이너를 자동으로 시작하도록 설정함
>>>2. docker ps 명령으로 생성한 컨테이너 상태를 확인함
>>>3. docker ps -f id=CONTAINER ID 명령으로 컨테이너를 지정하여 검색할 수 있음
>>>>- -f(--filter) [필터링대상] 옵션으로 검색 결과를 필터링 할 수 있으며, 필터링 대상을 지정할 때는 key(대상)=value(값) 형식으로 입력함
>>>4. curl 127.0.0.1 명령으로 마스터 노드 내부에 존재하는 nginx 컨테이너가 제공하는 nginx 웹 페이지 정보를 가져오려 하지만 Connection refused 오류가 발생함을 확인
>>>>- 마스터 노드의 입장에서는 웹 브라우저를 통한 접속은 기본 포트인 80번에서 처리하려고 하나 이에 대해 응답해 줄 주체가 없는 것임
>>>>- 응답을 컨테이너에서 처리해주기를 원한다면 80번으로 들어온 것을 컨테이너에서 받아줄 수 있는 포트로 연결해 주는 설정이 필요
>>>>- 정리하자면, curl 127.0.0.1 로 전달한 요청은 로컬호스트의 80번 포트로 전달만 될 뿐 컨테이너까지는 도달하지 못함. 즉, 호스트에 도달한 후 컨테이너로 도달하기 위한 추가 경로 설정이 필요함
>>- 추가로 경로를 설정해 정상적으로 컨테이너 실행하는 실습과정
>>>1. docker run -d -p 8080:80 --name nginx-exposed --restart always nginx 명령으로 컨테이너 외부에서도 컨테이너 내부에 접속할 수 있게 새로운 컨테이너를 구동함
>>>>- -p(--publish)는 외부에서 호스트로 보낸 요청을 컨테이너 내부로 전달하는 옵션으로 -p [요청 받을 호스트 포트]:[연결할 컨테이너 포트] 형식으로 사용함
>>>2. docker ps -f name=nginx-exposed 명령으로 컨테이너가 제대로 작동하는지 확인함
>>>>- 0.0.0.0:8080->80/tcp 는 존재하는 모든 네트워크 어댑터의 8080번 포트로 들어오는 요청을 컨테이너 내부의 80번 포트로 전달한다는 의미임
>>>>- m-k8s 호스트는 자기 자신을 나타내는 127.0.0.1과 외부에 노출된 192.168.1.10 등의 IP를 가지고 있는데, 요청이 호스트에 할당된 어떤 IP의 8080번 포트로 들어오더라도 컨테이너 내부의 80번 포트로 전달됨
>>>3. 192.168.1.10:8080 을 웹브라우저로 접속하여 가상 머신을 호스팅하는 PC나 노트북에서 컨테이너로 접근할 수 있는지 확인함
>- 컨테이너 내부 파일 변경하기
>>- 컨테이너 내부에서 컨테이너 외부의 파일을 사용할 수 있는 4가지 방법
>>>1. docker cp
>>>>- docker cp [호스트경로] [컨테이너이름]:[컨테이너 내부 경로] 형식으로 호스트에 위치한 파일을 구동 중인 컨테이너 내부에 복사함
>>>>- 컨테이너에 임시로 필요한 파일이 있는 경우 단편적으로 전송하기 위해 사용하거나 컨테이너에 저장되어 있는 설정 및 로그를 추출해 확인하는 목적으로도 사용함
>>>2. Dockerfile ADD
>>>>- 이미지는 Dockerfile을 기반으로 만들어지는데, 이때 Dockerfile에 ADD 구문으로 컨테이너 내부로 복사할 파일을 지정하면 이미지를 빌드할 때 지정한 파일이 이미지 내부로 복사됨
>>>>- 이후 해당 이미지를 기반으로 구동한 컨테이너는 복사한 파일을 사용할 수 있으나 사용자가 원하는 파일을 선택해 사용할 수 없다는 약점이 존재함
>>>3. 바인드 마운트
>>>>- 호스트의 파일 시스템과 컨테이너 내부를 연결해 어느 한쪽에서 작업한 내용이 양쪽에 동시에 반영되는 방법임
>>>>- 새로운 컨테이너를 구동할 때도 호스트와 연결할 파일이나 디렉토리의 경로만 지정하면 다른 컨테이너에 있는 파일을 새로 생성한 컨테이너와 연결할 수 있음
>>>>- DB의 데이터 디렉토리나 서버의 첨부 파일 디렉토리처럼 컨테이너가 바뀌어도 없어지면 안 되는 자료는 이 방법으로 보존할 수 있음
>>>4. 볼륨
>>>>- 볼륨은 도커가 직접 관리하며 컨테이너에 제공하는 호스트의 공간임
>>>>- 호스트의 파일 시스템과 컨테이너 내부를 연결하는 것은 바인드 마운트와 동일함
>>>>- 호스트의 특정 디렉토리가 아닌 도커가 관리하는 볼륨을 컨테이너와 연결함
>>>>- 도커가 관리하는 볼륨 공간을 NFS와 같은 공유 디렉토리에 생성한다면 다른 호스트에서도 도커가 관리하는 볼륨을 함께 사용할 수 있음
>>- 바인드 마운트로 호스트와 컨테이너 연결하는 실습
>>>1. mkdir -p /root/html 명령으로 호스트와 컨테이너를 연결할 디렉토리를 생성함
>>>2. docker run -d -p 8081:80 -v /root/html:/usr/share/nginx/html --restart always --name nginx-bind-mounts nginx 명령으로 컨테이너를 구동함과 동시에 컨테이너의 디렉토리와 호스트이 디렉토리를 연결함
>>>>- -v(--volume) 옵션은 호스트 디렉토리와 컨테이너 디렉토리를 연결하는 옵션으로, -v [호스트 디렉토리 경로]:[컨테이너 디렉토리 경로] 형식으로 사용함
>>>>- 바인드 마운트의 중요한 특성으로, 호스트 디렉토리의 내용을 그대로 컨테이너 디렉토리에 덮어쓰기 때문에 컨테이너 디렉토리에 어떠한 내용이 있더라도 해당 내용은 삭제된다는 점을 유의해야함
>>>3. docker ps -f name=nginx-bind-mounts 명령으로 컨테이너의 STATUS가 정상인지 확인함
>>>4. 웹 브라우저에서 192.168.1.10:8081 에 연결하여 nginx-bind-mounts 컨테이너에서 실행되는 nginx에 접속하여 오류 화면이 보이는지 확인함
>>>>- 사용자가 nginx 에 접속하면 index.html 을 읽어서 화면에 표시해 주도록 설계되어 있어 바인드 마운트 설정에 따라 호스트 디렉토리의 /root/html에 있는 index.html을 노출하려고 하지만 해당 파일이 존재하지 않기 떄문에 오류화면이 출력됨
>>>>- 403 Forbidden 오류는 권한이 없을때 표시되는 에러이지만, 파일이 존재하지 않을때도 표시됨
>>>5. cp ~/_Book_k8sInfra/ch4/4.2.3/index-BindMount.html /root/html/index.html 명령으로 파일을 복사하여 index.html을 만듬
>>>6. 웹 브라우저에서 192.168.1.10:8081 에 연결하여 index.html이 표시되는지 확인함
>>>7. docker exec [컨테이너 ID | 이름] [명령어] 형식으로 실행하면 컨테이너에서 명령을 실행하고 결과를 출력함
>>- 볼륨으로 호스트와 컨테이너 연결하는 실습
>>>1. docker volume create nginx-volume 명령으로 볼륨을 생성함
>>>2. docker volume inspect nginx-volume 명령으로 생성된 볼륨을 조회함
>>>>- 볼륨에 적용된 드라이버 종류와 실제 호스트에 연결된 디렉터리, 볼륨 이름 등을 조회할 수 있음
>>>>- 컨테이너 내부와 연결할 때 전체 디렉토리 경로를 사용하지 않고 nginx-volume 이라는 볼륨 이름만으로 간편하게 연결할 수 있음
>>>3. ls /var/lib/docker/volumes/nginx-volume/_data 명령으로 생성된 디렉토리를 확인함
>>>4. 기존 컨테이너는 설정을 바꿀 수 없으므로 docker run -d -v nginx-volume:/usr/share/nginx/html -p 8082:80 --restart always --name nginx-volume nginx 명령으로 호스트와 컨테이너의 디렉토리를 연결할 컨테이너를 구동함
>>>5. ls /var/lib/docker/volumes/nginx-volume/_data 명령으로 볼륨 디렉토리의 내용을 다시 확인함
>>>>- 바인드 마운트와 달리 볼륨은 빈 디렉토리를 덮어쓰지 않고 컨테이너 내부에 있는 50x.html과 index.html 파일을 보존함
>>>6. 192.168.1.10:8082 을 웹브라우저로 접속하여 nginx 의 index.html 이 표시되는지 확인
>>>>- 볼륨은 바인드 마운트와 달리 호스트 디렉토리를 컨테이너 디렉토리에 덮어쓰느 구조가 아니라 양쪽을 서로 동기화시키는 구조이기 때문에 비어 있는 볼륨을 연결하는 경우에는 컨테이너 디렉토리에 있는 파일이 보존됨
>>>>- 볼륨에 컨테이너 디렉토리와 동일한 파일이 존재한 상태로 연결하는 경우에는 덮어쓰기가 되므로 유의할 필요가 있음
>>>7. cp ~/_Book_k8sInfra/ch4/4.2.3/index-Volume.html /var/lib/docker/volumes/nginx-volume/_data/index.html 명령으로 바꿀 파일을 볼륨 디렉토리로 복사하여 볼륨에서 변경한 내용이 컨테이너 디렉토리에 동기화되는지 웹 브라우저에서 테스트함
>>>- 볼륨을 사용하면 컨테이너에 존재하는 파일을 그대로 보존할 수 있고, 필요할 때 변경해서 사용할 수도 있음
>>>- 사용 중인 볼륨을 docker volume ls 명령으로 조회할 수 있고, docker volume rm 명령으로 삭제할 수도 있어서 바인드 마운트보다 관리하기 더 쉬움
>- 사용하지 않는 컨테이너 정리하기
>>- 컨테이너를 정지하는 실습
>>>- 컨테이너나 이미지를 삭제하기 전에 먼저 컨테이너를 정지해야 하며 동일한 호스트의 포트를 사용하는 컨테이너를 배포하거나 작동 중인 컨테이너의 사용 자체를 종료할 때도 먼저 컨테이너를 정지해야 함
>>>1. docker ps -f ancestor=nginx 명령으로 nginx 이미지를 기반으로 생성된 컨테이너를 조회함
>>>2. docker stop [컨테이너 이름 | ID] 명령으로 4번째 행에 표시된 컨테이너를 정지함
>>>3. docker stop [컨테이너 이름 | ID] 명령으로 nginx-exposed 컨테이너의 ID를 이용하여 정지함
>>>4. docker ps -q -f ancestor=nginx 명령으로 nginx 이미지를 사용하는 모든 컨테이너의 ID만 출력함
>>>5. docker stop $(docker ps -q -f ancestor=nginx) 명령처럼 4번에서 사용한 명령을 인자로 사용하여 nginx를 이미지로 사용하는 모든 컨테이너를 정지함
>>>6. docker ps -f ancestor=nginx 명령으로 모든 컨테이너가 정지됐는지 확인함
>>>7. docker ps -a -f ancestor=nginx 명령으로 삭제된 것이 아니라 정지된 것임을 확인함
>>>8. docker start [컨테이너 이름 | ID] 명령으로 정지한 컨테이너를 다시 구동할 수 있음
>>- 컨테이너와 이미지를 삭제하는 실습
>>>1. docker rm $(docker ps -aq -f ancestor=nginx) 명령으로 컨테이너를 삭제함
>>>>- 정지하지 않은 컨테이너를 삭제 시 docker rm -f(--force) 명령으로 강제로 삭제할 수 있으나 의도하지 않은 삭제가 일어날 수 있음
>>>2. docker ps -a -f ancestor=nginx 명령으로 컨테이너가 정상적으로 삭제됐는지 확인함
>>>3. docker rmi $(docker images -q nginx) 명령으로 nginx:latest 이미지와 nginx:stable 이미지를 한 번에 삭제함
>>>>- 이미지는 컨테이너가 정지 상태가 아닌 삭제 상태일 때 삭제할 수 있음

<br>

[목차로 이동](#목차)

>### 4가지 방법으로 컨테이너 이미지 만들기
>- 개요
>>- 컨테이너 인프라 환경을 구성할 때 이미 제공된 이미지를 사용하는 경우도 있지만, 직접 만든 애플리케이션으로 컨테이너를 만들 수도 있음
>>- 해당 서적에서는 저자가 제공하는 소스 코드로 자바 실행 파일을 빌드하고 이를 다시 도커 빌드를 사용해 컨테이너 이미지로 만듬
>>- 컨테이너 이미지를 만드는 4가지 방법
>>>1. 기본적인 빌드
>>>2. 용량 줄이기
>>>3. 컨테이너 내부 빌드
>>>4. 멀티 스테이지
>- 기본 방법으로 빌드하기
>>- 개요
>>>- 컨테이너 이미지를 만드는 방법 중 가장 간단한 방법임
>>>- 원활한 실습을 위해 저자가 제공하는 스프링 부트를 이용해 만든 자바 소스 코드로 이미지를 빌드함
>>>- 진행할 컨테이너 이미지 빌드 과정은 자바 소스 빌드 -> 도커파일 작성 -> 도커파일 빌드 -> 빌드 완료 순서로 진행됨
>>- 기본 방법으로 빌드하기 실습
>>>1. cd ~/_Book_k8sInfra/ch4/4.3.1 명령으로 기본적인 컨테이너 빌드 도구와 파일이 있는 빌드 디렉토리로 이동함
>>>>- Dockerfile : 컨테이너 이미지를 빌드하기 위한 정보를 담고 있음
>>>>- mvnw : 메이븐 래퍼라는 이름의 리눅스 스크립트로, 메이븐 실행을 위한 환경 설정을 자동화함
>>>>- pom.xml : 메이븐 래퍼가 작동할 때 필요한 절차와 빌드 정보를 담고 있음
>>>>- src(디렉토리) : 메이븐으로 빌드할 자바 소스 디렉토리임
>>>2. yum install java-1.8.0-openjdk-devel -y 명령으로 자바 개발 도구(JDK)를 설치함
>>>3. chmod 700 mvnw 명령으로 실행 권한을 변경하고 ./mvnw clean package 명령으로 빌드를 진행할 디렉토리를 비우고 JAP를 생성함
>>>4. ls target 명령으로 생성된 JAR 파일을 확인함
>>>5. docker build -t basic-img . 명령으로 컨테이너 이미지를 빌드함
>>>>- -t(--tag)는 만들어질 이미지를 의미하고 .은 이미지에 원하는 내용을 추가하거나 변경하는 데 필요한 작업 공간을 현재 디렉토리로 지정한다는 의미임
>>>>- Dockerfile 은 도커로 컨테이너를 빌드하는 핵심 부분임
>>>>```Dockerfile
>>>>FROM openjdk:8
>>>>LABEL description="Echo IP Java Application"
>>>>EXPOSE 60431
>>>>COPY ./target/app-in-host.jar /opt/app-in-image.jar
>>>>WORKDIR /opt
>>>>ENTRYPOINT [ "java", "-jar", "app-in-image.jar" ]
>>>>```
>>>>>- 1번째 줄 FROM [이미지이름]:[태그] 형식으로 이미지를 가져옴
>>>>>>- 가져온 이미지 내부에서 컨테이너 이미지를 빌드함
>>>>>>- 누군가 만들어 놓은 이미지에 필요한 부분을 추가하는것과 같음
>>>>>>- 여기선 openjdk를 기초 이미지로 사용하는데, 기초 이미지로 어떤 것을 선택하냐에 따라 다양한 환경의 컨테이너를 빌드할 수 있음
>>>>>- 2번째 줄 LABEL [레이블 이름]=[값] 형식으로 이미지에 부가적인 설명을 위한 레이블을 추가할 때 사용함
>>>>>- 3번째 줄 EXPOSE [숫자] 형식으로 생성된 이미지로 컨테이너를 구동할 때 어떤 포트를 사용하는지 알려줌
>>>>>>- EXPOSE를 사용한다고 해서 컨테이너를 구동할 때 자동으로 해당 포트를 호스트 포트와 연결하지 않음
>>>>>>- 외부와 연결하려면 지정한 포트를 호스트 포트와 연결해야 한다는 정보를 제공할 뿐임
>>>>>>- 실제로 외부에서 접속하려면 docker run 으로 이미지를 컨테이너로 빌드할 때 -p 옵션을 넣어 포트를 연결해야 함
>>>>>- 4번째 줄 COPY [호스트 경로] [컨테이너 경로] 형식으로 호스트에서 새로 생성하는 컨테이너 이미지로 필요한 파일을 복사함
>>>>>>- 메이븐을 통해 생성한 app-in-host.jar 파일을 이미지의 /opt/app-in-image.jar로 복사함
>>>>>- 5번째 줄 이미지의 현재 작업 위치를 opt로 변경함
>>>>>- 6번째 줄 ENTRYPOINT ["명령어", "옵션", ... , "옵션"] 의 형식으로 컨테이너 구동 시 ENTRYPOINT 뒤에 나오는 대괄호 안에 든 명령을 실행함
>>>>>>- ENTRYPOINT로 실행하는 명령어는 컨테이너를 구동할 때 첫 번째로 실행되며 이 명령어로 실행된 프로세스는 컨테이너 내부에서 첫 번째로 실행됐다는 의미로 PID는 1이 됨
>>>6. docker images basic-img 명령으로 생성한 이미지를 확인함
>>>7. docker build -t basic-img:1.0 -t basic-img:2.0 . 명령으로 -t 옵션으로 1.0과 2.0 태그의 이미지도 생성함, 캐시가 사용되어 매우 빠르게 빌드됨
>>>8. docker images basic-img 명령으로 생성된 이미지를 확인하여 이미지가 모두 용량이 같은것을 확인함
>>>>- 이미지들은 태그 정보만 다를 뿐 모두 같은 이미지이며, 한 공간을 사용함
>>>9. sed -i 's/Application/Development/' Dockerfile 명령으로 내용의 일부를 변경하고 docker build -t basic-img:3.0 . 명령으로 다시 빌드함
>>>10. docker imges basic-img 명령으로 생성된 이미지를 확인하여 완전히 다른 ID 의 이미지가 생성된 것을 확인함
>>>11. docker run -d -p 60431:80 --name basic-run --restart always basic-img 명령으로 컨테이너를 실행하고 docker ps -f name=basic-run 명령으로 컨테이너 상태를 출력하여 생성한 컨테이너 이미지가 컨테이너로 작동하는지 확인함
>>>12. curl 127.0.0.1:60431 명령으로 컨테이너가 정상적으로 외부 요청에 응답하는지 확인함
>>>13. docker rm -f basic-run 명령으로 작동 중인 컨테이너를 바로 삭제함
>>>14. docker rmi -f $(docker images -q basic-img) 명령으로 빌드한 컨테이너 이미지를 모두 삭제함
>>>15. docker build -t basic-img . 명령으로 다음 절에서 빌드할 이미지와 용량을 비교하기 위해 컨테이너 이미지를 하나 다시 빌드함
>- 컨테이너 용량 줄이기
>>- 개요
>>>- 진행할 컨테이너 이미지의 빌드 과정은 도커파일 작성 -> 도커파일 빌드 -> 빌드 완료로 기본 방법보다 1단계가 줄고, 기초 이미지가 openjdk에서 GCR(Google Container Registry)에서 제공하는 distroless로 변경됨
>>- 컨테이너 용량 줄여 빌드하기 실습
>>>1.  cd ~/_Book_k8sInfra/ch4/4.3.2 명령으로 이동 후 ls 명령으로 build-in-host.sh 파일이 있음을 확인
>>>2. cat build-in-host.sh 명령으로 기본방법에서 진행한 내용을 스크립트로 작성한 것임을 확인함
>>>3. cat Dockerfile 명령으로 내용을 확인함
>>>>- 사용되는 기초 이미지가 openjdk 에서 gcr.io/distroless/java:8 으로 변경됨
>>>>- distroless는 자바 실행을 위해 경량화된 이미지임
>>>>- 기본 방법에서는 openjdk 이미지를 설치할 때 호스트에 자바 개발 도구인 java-1.8.0-openjdk-devel을 함께 설치하여 불필요한 공간 낭비를 발생시킴
>>>4. chmod 700 mvnw 명령으로 실행권한을 부여함
>>>5. ./build-in-host.sh 명령으로 경량화 이미지를 빌드함
>>>6. docker images | head -n 3 명령으로 기본 방법으로 빌드한 이미지와 용량을 줄여 빌드한 컨테이너 이미지의 용량을 비교함
>>>7. docker run -d -p 60432:80 --name optimal-run --restart always optimal-img 명령으로 생성한 컨테이너 이미지를 실행하고 curl 127.0.0.1:60432 로 확인함
>>>8. docker rm -f optimal-run 으로 빌드한 컨테이너를 삭제함
>- 컨테이너 내부에서 컨테이너 빌드하기
>>- 개요
>>>- 번거로운 과정 없이 바로 자바 소스를 컨테이너 이미지에서 빌드하면 어떻게 되는지 확인함
>>>- 컨테이너 내부에서 컨테이너를 빌드하는 과정은 도커파일 작성 -> 도커파일 빌드 -> 빌드완료 순서임
>>- 컨테이너 내부에서 빌드하기 실습
>>>1. cd ~/_Book_k8sInfra/ch4/4.3.3 명령으로 이동하여 ls 명령으로 Dockerfile 하나만 있음을 확인함
>>>>- 빌드 과정 자체를 openjdk 이미지에서 진행하므로 나머지는 필요없어짐
>>>2. cat Dockerfile 명령으로 내용을 확인함
>>>3. git clone https://github.com/iac-source/inbuilder.git 명령으로 이미지 내부에서 내려받는 저장소의 구조를 확인함
>>>4. docker build -t nohost-img . 명령으로 컨테이너 이미지를 빌드함
>>>5. docker images | head -n 4 명령으로 기존 이미지들과 비교하여 가장 용량이 큰 것을 확인함
>>>>- 컨테이너 내부에서 빌드를 진행하기 때문에 빌드 중간에 생성한 파일들과 내려받은 라이브러리 캐시들이 최종 이미지에 그대로 남기때문에 더 커짐
>>>6. docker run -d -p 60433:80 --name nohost-run --restart always nohost-img 명령으로 컨테이너를 실행하고 curl 127.0.0.1:60433 ㅡ올 동작을 확인함
>>>7. docker rm -f nohost-run 명령으로 삭제함
>- 최적화해 컨테이너 빌드하기
>>- 개요
>>>-  멀티 스테이지 빌드방법은 최종 이미지의 용량을 줄일 수 있고 호스트에 어떠한 빌드 도구도 설치할 필요가 없음
>>>- 멀티 스테이지를 이용한 컨테이너 이미지 빌드 과정은 도커파일 작성 -> 도커파일 빌드 -> 빌드 완료 순서임
>>>- 멀티 스테이지는 docker-ce 17.06 버전부터 지원되어 현재 사용중인 도커 버전을 업그레이드 해야하므로 사용 중인 쿠버네티스 클러스터를 삭제하고 새로운 쿠버네티스 클러스터를 만들어야 함
>>- 멀티 스테이지 빌드 실습
>>>1. kubectl get nodes -o wide 명령으로 현재 사용하는 도커 버전을 확인함
>>>2. cmd 에서 cd c:\HashiCorp\_Book_k8sInfra-main\ch3\3.1.3 명령으로 이동하여 vagrant destroy -f 명령으로 기존 가상머신들을 제거함
>>>3. cd c:\HashiCorp\_Book_k8sInfra-main\ch4\4.3.4\k8s-SingleMaster-18.9_9_w_auto-compl 명령으로 이동하여 vagrant up 명령을 실행하여 멀티 스테이지를 지원하는 버전의 도커가 포함된 새로운 쿠버네티스 클러스터 환경을 구성함
>>>4. 가상 머신 구성이 완료되면 슈퍼푸티로 재접속함
>>>5. kubectl get nodes -o wide 명령으로 도커 버전을 확인함
>>>6. cd ~/_Book_k8sIngra/ch4/4.3.4/ 명령으로 이동 후 ls 명령으로 Dockerfile이 있는지 확인함
>>>7. cat Dockerfile 명령으로 내용을 확인함
>>>>- 멀티 스테이지의 핵심은 빌드하는 위치와 최종 이미지를 분리하는 것이므로 최종 이미지는 빌드된 JAR을 가지고 있지만, 용량은 줄일 수 있음
>>>8. docker build -t multistage-img 명령으로 멀티 스테이지 방식으로 작성된 Dockerfile로 컨테이너 이미지를 빌드함
>>>9. docker images | head -n 3 명령으로 빌드된 컨테이너 이미지의 용량을 확인함
>>>10. 컨테이너 이미지 중 none 으로 표시되는 이미지는 이름이 없는 이미지로 댕글링(dangling) 이미지라고 함
>>>>- 멀티 스테이지 과정에서 자바 소스를 빌드하는 과정에 생성된 이미지임
>>>>- 공간을 적게 사용하는 이미지를 만드는 것이 목적이므로 docker rmi $(docker images -f dangling=true -q) 명령으로 댕글링 이미지를 삭제함
>>>11. docker run -d -p 60434:80 --name multistage-run --restart always multistage-img 명령으로 컨테이너를 생성하고 curl 127.0.0.1:60434 명령으로 동작을 확인함
>>>12. docker rm -f multistage-run 명령으로 컨테이너를 삭제하고 다음 실습을 위해 cd ~ 명령으로 홈 디렉토리로 이동함

<br>

[목차로 이동](#목차)

>### 쿠버네티스에서 직접 만든 컨테이너 사용하기
>- 쿠버네티스에서 도커 이미지 구동하기
>>- 쿠버네티스에서 컨테이너 이미지 구동하기 실습
>>>1. kubectl create deployment failure1 --image=multistage-img 명령으로 디플로이먼트를 생성함
>>>2. kubectl get pods -w 명령으로 파드의 상태 및 변화를 확인하면 이미지를 내려받는데 문제가 발생하여 ErrorImagePull 과 ImagePullBackOff 라는 오류 메시지가 번갈아 표시됨
>>>>- 이미지가 호스트에 존재하지만 기본 설정에 따라 외부(도커 허브)에서 받으려고 시도하기 때문임
>>>3. 내부에 존재하는 컨테이너 이미지를 사용하도록 설정해서 디플로이먼트를 생성해야함
>>>>- 사용자가 원하는 형태의 디플로이먼트를 만드는 가장 좋은 방법은 현재 수행되는 구문을 야믈 형태로 뽑아내는 것임
>>>>>- kubectl create deployment failure2 --dry-run=client -o yaml --image=multistage-img > failure2.yaml 명령을 실행함
>>>>- --dry-run=client 옵션은 해당 내용을 실제로 적용하지 않은채 명령을 수행하고 -o yaml 은 현재 수행되는 명령을 야믈 형태로 바꾸는 옵션인데 두 옵션을 조합하면 현재 수행되는 명령을 야믈 형태로 출력해 사용자가 원하는 형태로 변경할 수 있음
>>>>- 마지막에 > failure2.yaml 옵션으로 실행 결과를 파일로 저장함
>>>4. vi failure2.yaml 명령으로 파일을 열어서 내용을 수정함
>>>```yaml
>>>spec:
>>>      containers:
>>>      - image: multistage-img
>>>        imagePullPolicy: Never
>>>        name: multistage-img
>>>        resources: {}
>>>```
>>>>- imagePullPolicy: Never 옵션은 외부에서 이미지를 가져오지 않고 호스트에 존재하는 이미지를 사용하게 함
>>>5. kubectl apply -f failure2.yaml 명령으로 디플로이먼트에 적용하고 kubectl get pods 명령으로 파드의 상태를 확인함
>>>>- 형태는 바뀌었지만 여전히 오류가 발생함을 확인함
>>>6. kubectl delete deployment failure1 명령과 kubectl delete deployment failure2 명령으로 오류가 발생하는 디플로이먼트를 모두 삭제함
>>>7. 슈퍼푸티로 워커 노드 w3-k8s로 접속하여 curl -O https://raw.githubusercontent.com/sysnet4admin/_Book_k8sInfra/main/ch4/4.3.4/Dockerfile 명령을 실행하여 저자가 깃허브에 올려 둔 Dockerfile 을 받아와서 테스트를 위한 컨테이너 이미지를 만듬
>>>8. docker build -t multistage-img . 명령으로 컨테이너 이미지를 빌드하고 결과가 성공적으로 이루어졌는지 화인함
>>>9. 마스터 노드에서 cp failure2.yaml success1.yaml 명령으로 복사함
>>>10. sed -i 's/replicas: 1/replicas: 3/' success1.yaml 명령과 sed -i 's/failure2/success1/' success1.yaml 명령으로 내용을 변경함
>>>11. kubectl apply -f success1.yaml 명령으로 실행하고 kubectl get pods -o wide 명령으로 배포에 성공한 노드가 워커 3번인지 확인함
>>>>- 컨테이너 이미지가 워커 노드 3번에만 있기때문에 다른 워커 노드에는 파드를 생성할 수 없음
>>>>- 해결 방법으로 도커 허브에 multistage-img를 올려서 다시 내려받거나 쿠버네티스 클러스터가 접근할 수 있는 곳에 이미지 레지스트리를 만들고 그곳에서 받아오도록 설정하는 것임
>>>12. kubectl delete -f success1.yaml 명령으로 배포한 디플로이먼트를 삭제함
>>>13. docker rmi multistage-img 명령과 docker rmi $(docker images -f dangling=true -q) 명령으로 테스트를 위해 워커 노드 3번에 생성한 컨테이너 이미지와 댕글링 이미지도 삭제함
>- 레지스트리 구성하기
>>- 개요
>>>- 호스트에서 생성한 이미지를 쿠버네티스에서 사용하려면 모든 노드에서 공통으로 접근하는 레지스트리(저장소)가 필요함
>>>- 도커나 쿠버네티스는 인터넷이 연결돼 있다면 도커 허브라는 레지스트리에서 이미지를 내려받을 수 있음
>>>- 직접 만든 이미지를 외부에 비공개하려면 도커 허브에서 제공하는 사설 저장소가 있지만 무료 사용자는 1개만 사용할 수 있으며 이미지를 내려받는 횟수에 제약이 있음
>>>- 제약 없이 사용할 수 있는 저장소가 필요할 경우 레지스트리를 직접 구축하는 방법이 있으며 인터넷을 연결할 필요가 없으므로 보안이 중요한 내부 전산망에서도 구현이 가능함
>>>- 도커에서 제공하는 도커 레지스트리 이미지를 사용해 사설 도커 레지스트리를 만들 수 있음
>>>>- 기능은 부족하지만 컨테이너를 하라만 구동하면 돼서 설치가 간편하고 내부에서 테스트 목적으로 사용하기에 적합함
>>- 사설 도커 레지스트리 만들기 실습
>>>1. ls ~/_Book_k8sInfra/ch4/4.4.2 명령으로 사설 이미지 레지스트리 구성을 위한 파일들을 확인함
>>>>- 인증서를 만들어 배포한 뒤 레지스트리를 구동하는 create-registry.sh 파일과 인증서를 만들 때 사용하는 tls.csr 파일이 있음
>>>>- 인증서를 생성하려면 서명 요청서(CSR)를 작성해야 하며 서명 요청서에는 인증키를 생성하는 개인이나 기관의 정보와 인증서를 생성하는 데 필요한 몇 가지 추가 정보를 기록한 후 CSR을 기반으로 인증서와 개인키를 생성하는데, 이때 사용하는 CSR이 tls.csr 파일임
>>>>- remover.sh 는 인증 문제가 생겼을 때 모든 설정을 지우는 스크립트임
>>>>- 웹 서버에서 사용하는 인증서를 사용할 때는 서명 요청서 정보 없이 명령줄에서 직접 인증서를 생성하지만 도커는 이미지를 올리거나 내려받으려고 레지스트리에 접속하는 과정에서 주체 대체 이름(SAN)이라는 추가 정보를 검증하기 때문에 요청서에 추가 정보를 기입해 인증서를 생성하는 과정이 필요함
>>- ... 생략

<br>

[목차로 이동](#목차)

---

## 지속적 통합과 배포 자동화, 젠킨스

>### 컨테이너 인프라 환경에서 CI/CD
>- 컨테이너 인프라 환경에서 CI/CD
>>- 개요
>>>- CI는 코드를 커밋하고 빌드했을 때 정상적으로 작동하는지 반복적으로 검증해 애플리케이션의 신뢰성을 높이는 작업임
>>>- CD는 CI 과정에서 생성된 신뢰할 수 있는 애플리케이션을 실제 상용 환경에 자동으로 배포하는 것을 의미함
>>>- 개발 과정에서의 CI/CD
>>>>1. 개발자가 소스를 커밋하고 푸시하면 CI 단계로 진입함
>>>>2. CI 단계에서 애플리케이션이 자동 빌드되고 테스트를 거쳐 배포할 수 있는 애플리케이션인지 확인함
>>>>3. 테스트를 통과하면 신뢰할 수 있는 애플리케이션으로 간주하고 CD 단계로 진입함
>>>>4. CD 단계에서는 애플리케이션을 컨테이너 이미지로 만들어서 파드, 디플로이먼트, 스테이트풀셋 등 다양한 오브젝트 조건에 맞춰 미리 설정한 파일을 통해 배포함

<br>

[목차로 이동](#목차)

>### 젠킨스 설치를 위한 간편화 도구 살펴보기
>- 

<br>

[목차로 이동](#목차)

>### 젠킨스 설치 및 설정하기
>- 

<br>

[목차로 이동](#목차)

>### 젠킨스로 CI/CD 구현하기
>- 

<br>

[목차로 이동](#목차)

>### 젠킨스 플러그인을 통해 구현되는 GitOps
>- 

<br>

[목차로 이동](#목차)

---